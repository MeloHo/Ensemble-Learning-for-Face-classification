{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Resnet for face recognition\n",
    "\n",
    "MLAI Project\n",
    "\n",
    "yidongh@andrew.cmu.edu\n",
    "\n",
    "2019-11-28\n",
    "'''\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "from torch.nn import init\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1, \n",
    "            padding=1, bias=True, groups=1):    \n",
    "    \"\"\"3x3 convolution with padding\n",
    "    \"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_channels, \n",
    "        out_channels, \n",
    "        kernel_size=3, \n",
    "        stride=stride,\n",
    "        padding=padding,\n",
    "        bias=bias,\n",
    "        groups=groups)\n",
    "\n",
    "\n",
    "def conv1x1(in_channels, out_channels, groups=1):\n",
    "    \"\"\"1x1 convolution with padding\n",
    "    - Normal pointwise convolution When groups == 1\n",
    "    - Grouped pointwise convolution when groups > 1\n",
    "    \"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_channels, \n",
    "        out_channels, \n",
    "        kernel_size=1, \n",
    "        groups=groups,\n",
    "        stride=1)\n",
    "\n",
    "\n",
    "def channel_shuffle(x, groups):\n",
    "    batchsize, num_channels, height, width = x.data.size()\n",
    "\n",
    "    channels_per_group = num_channels // groups\n",
    "    \n",
    "    # reshape\n",
    "    x = x.view(batchsize, groups, \n",
    "        channels_per_group, height, width)\n",
    "\n",
    "    # transpose\n",
    "    # - contiguous() required if transpose() is used before view().\n",
    "    #   See https://github.com/pytorch/pytorch/issues/764\n",
    "    x = torch.transpose(x, 1, 2).contiguous()\n",
    "\n",
    "    # flatten\n",
    "    x = x.view(batchsize, -1, height, width)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class ShuffleUnit(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, groups=3,\n",
    "                 grouped_conv=True, combine='add'):\n",
    "        \n",
    "        super(ShuffleUnit, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.grouped_conv = grouped_conv\n",
    "        self.combine = combine\n",
    "        self.groups = groups\n",
    "        self.bottleneck_channels = self.out_channels // 4\n",
    "\n",
    "        # define the type of ShuffleUnit\n",
    "        if self.combine == 'add':\n",
    "            # ShuffleUnit Figure 2b\n",
    "            self.depthwise_stride = 1\n",
    "            self._combine_func = self._add\n",
    "        elif self.combine == 'concat':\n",
    "            # ShuffleUnit Figure 2c\n",
    "            self.depthwise_stride = 2\n",
    "            self._combine_func = self._concat\n",
    "            \n",
    "            # ensure output of concat has the same channels as \n",
    "            # original output channels.\n",
    "            self.out_channels -= self.in_channels\n",
    "        else:\n",
    "            raise ValueError(\"Cannot combine tensors with \\\"{}\\\"\" \\\n",
    "                             \"Only \\\"add\\\" and \\\"concat\\\" are\" \\\n",
    "                             \"supported\".format(self.combine))\n",
    "\n",
    "        # Use a 1x1 grouped or non-grouped convolution to reduce input channels\n",
    "        # to bottleneck channels, as in a ResNet bottleneck module.\n",
    "        # NOTE: Do not use group convolution for the first conv1x1 in Stage 2.\n",
    "        self.first_1x1_groups = self.groups if grouped_conv else 1\n",
    "\n",
    "        self.g_conv_1x1_compress = self._make_grouped_conv1x1(\n",
    "            self.in_channels,\n",
    "            self.bottleneck_channels,\n",
    "            self.first_1x1_groups,\n",
    "            batch_norm=True,\n",
    "            relu=True\n",
    "            )\n",
    "\n",
    "        # 3x3 depthwise convolution followed by batch normalization\n",
    "        self.depthwise_conv3x3 = conv3x3(\n",
    "            self.bottleneck_channels, self.bottleneck_channels,\n",
    "            stride=self.depthwise_stride, groups=self.bottleneck_channels)\n",
    "        self.bn_after_depthwise = nn.BatchNorm2d(self.bottleneck_channels)\n",
    "\n",
    "        # Use 1x1 grouped convolution to expand from \n",
    "        # bottleneck_channels to out_channels\n",
    "        self.g_conv_1x1_expand = self._make_grouped_conv1x1(\n",
    "            self.bottleneck_channels,\n",
    "            self.out_channels,\n",
    "            self.groups,\n",
    "            batch_norm=True,\n",
    "            relu=False\n",
    "            )\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _add(x, out):\n",
    "        # residual connection\n",
    "        return x + out\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _concat(x, out):\n",
    "        # concatenate along channel axis\n",
    "        return torch.cat((x, out), 1)\n",
    "\n",
    "\n",
    "    def _make_grouped_conv1x1(self, in_channels, out_channels, groups,\n",
    "        batch_norm=True, relu=False):\n",
    "\n",
    "        modules = OrderedDict()\n",
    "\n",
    "        conv = conv1x1(in_channels, out_channels, groups=groups)\n",
    "        modules['conv1x1'] = conv\n",
    "\n",
    "        if batch_norm:\n",
    "            modules['batch_norm'] = nn.BatchNorm2d(out_channels)\n",
    "        if relu:\n",
    "            modules['relu'] = nn.ReLU()\n",
    "        if len(modules) > 1:\n",
    "            return nn.Sequential(modules)\n",
    "        else:\n",
    "            return conv\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # save for combining later with output\n",
    "        residual = x\n",
    "\n",
    "        if self.combine == 'concat':\n",
    "            residual = F.avg_pool2d(residual, kernel_size=3, \n",
    "                stride=2, padding=1)\n",
    "\n",
    "        out = self.g_conv_1x1_compress(x)\n",
    "        out = channel_shuffle(out, self.groups)\n",
    "        out = self.depthwise_conv3x3(out)\n",
    "        out = self.bn_after_depthwise(out)\n",
    "        out = self.g_conv_1x1_expand(out)\n",
    "        \n",
    "        out = self._combine_func(residual, out)\n",
    "        return F.relu(out)\n",
    "\n",
    "\n",
    "class ShuffleNet(nn.Module):\n",
    "    \"\"\"ShuffleNet implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, groups=1, in_channels=3, num_classes=2300):\n",
    "        \"\"\"ShuffleNet constructor.\n",
    "        Arguments:\n",
    "            groups (int, optional): number of groups to be used in grouped \n",
    "                1x1 convolutions in each ShuffleUnit. Default is 3 for best\n",
    "                performance according to original paper.\n",
    "            in_channels (int, optional): number of channels in the input tensor.\n",
    "                Default is 3 for RGB image inputs.\n",
    "            num_classes (int, optional): number of classes to predict. Default\n",
    "                is 1000 for ImageNet.\n",
    "        \"\"\"\n",
    "        super(ShuffleNet, self).__init__()\n",
    "\n",
    "        self.groups = groups\n",
    "        self.stage_repeats = [3, 7, 3]\n",
    "        self.in_channels =  in_channels\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # index 0 is invalid and should never be called.\n",
    "        # only used for indexing convenience.\n",
    "        if groups == 1:\n",
    "            self.stage_out_channels = [-1, 24, 144, 288, 512]\n",
    "        elif groups == 2:\n",
    "            self.stage_out_channels = [-1, 24, 200, 400, 800]\n",
    "        elif groups == 3:\n",
    "            self.stage_out_channels = [-1, 24, 240, 480, 960]\n",
    "        elif groups == 4:\n",
    "            self.stage_out_channels = [-1, 24, 272, 544, 1088]\n",
    "        elif groups == 8:\n",
    "            self.stage_out_channels = [-1, 24, 384, 768, 1536]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"\"\"{} groups is not supported for\n",
    "                   1x1 Grouped Convolutions\"\"\".format(num_groups))\n",
    "        \n",
    "        # Stage 1 always has 24 output channels\n",
    "        self.conv1 = conv1x1(self.in_channels,\n",
    "                             self.stage_out_channels[1]) # stage 1\n",
    "                             \n",
    "        #self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Stage 2\n",
    "        self.stage2 = self._make_stage(2)\n",
    "        # Stage 3\n",
    "        self.stage3 = self._make_stage(3)\n",
    "        # Stage 4\n",
    "        self.stage4 = self._make_stage(4)\n",
    "\n",
    "        # Global pooling:\n",
    "        # Undefined as PyTorch's functional API can be used for on-the-fly\n",
    "        # shape inference if input size is not ImageNet's 224x224\n",
    "\n",
    "        # Fully-connected classification layer\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        num_inputs = self.stage_out_channels[-1]\n",
    "        self.fc = nn.Linear(num_inputs, self.num_classes)\n",
    "        self.init_params()\n",
    "\n",
    "\n",
    "    def init_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "    def _make_stage(self, stage):\n",
    "        modules = OrderedDict()\n",
    "        stage_name = \"ShuffleUnit_Stage{}\".format(stage)\n",
    "        \n",
    "        # First ShuffleUnit in the stage\n",
    "        # 1. non-grouped 1x1 convolution (i.e. pointwise convolution)\n",
    "        #   is used in Stage 2. Group convolutions used everywhere else.\n",
    "        grouped_conv = stage > 2\n",
    "        \n",
    "        # 2. concatenation unit is always used.\n",
    "        first_module = ShuffleUnit(\n",
    "            self.stage_out_channels[stage-1],\n",
    "            self.stage_out_channels[stage],\n",
    "            groups=self.groups,\n",
    "            grouped_conv=grouped_conv,\n",
    "            combine='concat'\n",
    "            )\n",
    "        modules[stage_name+\"_0\"] = first_module\n",
    "\n",
    "        # add more ShuffleUnits depending on pre-defined number of repeats\n",
    "        for i in range(self.stage_repeats[stage-2]):\n",
    "            name = stage_name + \"_{}\".format(i+1)\n",
    "            module = ShuffleUnit(\n",
    "                self.stage_out_channels[stage],\n",
    "                self.stage_out_channels[stage],\n",
    "                groups=self.groups,\n",
    "                grouped_conv=True,\n",
    "                combine='add'\n",
    "                )\n",
    "            modules[name] = module\n",
    "\n",
    "        return nn.Sequential(modules)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        #x = self.maxpool(x)\n",
    "\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "\n",
    "        # global average pooling layer\n",
    "        x = self.avgpool(x)\n",
    "        \n",
    "        # flatten for input to fully-connected layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, file_list, target_list):\n",
    "        self.file_list = file_list\n",
    "        self.target_list = target_list\n",
    "        self.n_class = len(list(set(target_list)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.file_list[index])\n",
    "        img = torchvision.transforms.ToTensor()(img)\n",
    "        label = self.target_list[index]\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_data(datadir):\n",
    "    img_list = []\n",
    "    ID_list = []\n",
    "    uniqueID_list = []\n",
    "    for root, directories, filenames in os.walk(datadir):\n",
    "        b = -1\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.jpg'):\n",
    "                if(filename[0]+filename[1]=='._'):\n",
    "                    filename = filename[2:]\n",
    "                filei = os.path.join(root, filename)\n",
    "                img_list.append(filei)\n",
    "                ID_list.append(root.split('/')[-1])\n",
    "                a = root.split('/')[-1]\n",
    "                if a!=b:\n",
    "                    uniqueID_list.append(a)\n",
    "                b=a\n",
    "                \n",
    "    class_n = len(uniqueID_list)\n",
    "    target_dict = dict(zip(uniqueID_list, range(class_n)))\n",
    "    reverse_dict = dict(zip(range(class_n),uniqueID_list))\n",
    "    label_list = [target_dict[ID_key] for ID_key in ID_list]\n",
    "    \n",
    "    print('{}\\t\\t{}\\n{}\\t\\t{}'.format('#Images', '#Labels', len(img_list), len(set(label_list))))\n",
    "    return img_list, label_list, class_n, reverse_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShuffleNet(\n",
       "  (conv1): Conv2d(3, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (stage2): Sequential(\n",
       "    (ShuffleUnit_Stage2_0): ShuffleUnit(\n",
       "      (g_conv_1x1_compress): Sequential(\n",
       "        (conv1x1): Conv2d(24, 36, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (depthwise_conv3x3): Conv2d(36, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=36)\n",
       "      (bn_after_depthwise): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (g_conv_1x1_expand): Sequential(\n",
       "        (conv1x1): Conv2d(36, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (ShuffleUnit_Stage2_1): ShuffleUnit(\n",
       "      (g_conv_1x1_compress): Sequential(\n",
       "        (conv1x1): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (depthwise_conv3x3): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36)\n",
       "      (bn_after_depthwise): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (g_conv_1x1_expand): Sequential(\n",
       "        (conv1x1): Conv2d(36, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (ShuffleUnit_Stage2_2): ShuffleUnit(\n",
       "      (g_conv_1x1_compress): Sequential(\n",
       "        (conv1x1): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (depthwise_conv3x3): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36)\n",
       "      (bn_after_depthwise): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (g_conv_1x1_expand): Sequential(\n",
       "        (conv1x1): Conv2d(36, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (ShuffleUnit_Stage2_3): ShuffleUnit(\n",
       "      (g_conv_1x1_compress): Sequential(\n",
       "        (conv1x1): Conv2d(144, 36, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (depthwise_conv3x3): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36)\n",
       "      (bn_after_depthwise): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (g_conv_1x1_expand): Sequential(\n",
       "        (conv1x1): Conv2d(36, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (stage3): Sequential(\n",
       "    (ShuffleUnit_Stage3_0): ShuffleUnit(\n",
       "      (g_conv_1x1_compress): Sequential(\n",
       "        (conv1x1): Conv2d(144, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (depthwise_conv3x3): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72)\n",
       "      (bn_after_depthwise): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (g_conv_1x1_expand): Sequential(\n",
       "        (conv1x1): Conv2d(72, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (ShuffleUnit_Stage3_1): ShuffleUnit(\n",
       "      (g_conv_1x1_compress): Sequential(\n",
       "        (conv1x1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (depthwise_conv3x3): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72)\n",
       "      (bn_after_depthwise): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (g_conv_1x1_expand): Sequential(\n",
       "        (conv1x1): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (ShuffleUnit_Stage3_2): ShuffleUnit(\n",
       "      (g_conv_1x1_compress): Sequential(\n",
       "        (conv1x1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (depthwise_conv3x3): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72)\n",
       "      (bn_after_depthwise): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (g_conv_1x1_expand): Sequential(\n",
       "        (conv1x1): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (ShuffleUnit_Stage3_3): ShuffleUnit(\n",
       "      (g_conv_1x1_compress): Sequential(\n",
       "        (conv1x1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (depthwise_conv3x3): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72)\n",
       "      (bn_after_depthwise): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (g_conv_1x1_expand): Sequential(\n",
       "        (conv1x1): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (ShuffleUnit_Stage3_4): ShuffleUnit(\n",
       "      (g_conv_1x1_compress): Sequential(\n",
       "        (conv1x1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (depthwise_conv3x3): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72)\n",
       "      (bn_after_depthwise): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (g_conv_1x1_expand): Sequential(\n",
       "        (conv1x1): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (ShuffleUnit_Stage3_5): ShuffleUnit(\n",
       "      (g_conv_1x1_compress): Sequential(\n",
       "        (conv1x1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (depthwise_conv3x3): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72)\n",
       "      (bn_after_depthwise): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (g_conv_1x1_expand): Sequential(\n",
       "        (conv1x1): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (ShuffleUnit_Stage3_6): ShuffleUnit(\n",
       "      (g_conv_1x1_compress): Sequential(\n",
       "        (conv1x1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (depthwise_conv3x3): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72)\n",
       "      (bn_after_depthwise): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (g_conv_1x1_expand): Sequential(\n",
       "        (conv1x1): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (ShuffleUnit_Stage3_7): ShuffleUnit(\n",
       "      (g_conv_1x1_compress): Sequential(\n",
       "        (conv1x1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (depthwise_conv3x3): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72)\n",
       "      (bn_after_depthwise): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (g_conv_1x1_expand): Sequential(\n",
       "        (conv1x1): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (stage4): Sequential(\n",
       "    (ShuffleUnit_Stage4_0): ShuffleUnit(\n",
       "      (g_conv_1x1_compress): Sequential(\n",
       "        (conv1x1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (depthwise_conv3x3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
       "      (bn_after_depthwise): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (g_conv_1x1_expand): Sequential(\n",
       "        (conv1x1): Conv2d(128, 224, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (ShuffleUnit_Stage4_1): ShuffleUnit(\n",
       "      (g_conv_1x1_compress): Sequential(\n",
       "        (conv1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (depthwise_conv3x3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "      (bn_after_depthwise): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (g_conv_1x1_expand): Sequential(\n",
       "        (conv1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (ShuffleUnit_Stage4_2): ShuffleUnit(\n",
       "      (g_conv_1x1_compress): Sequential(\n",
       "        (conv1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (depthwise_conv3x3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "      (bn_after_depthwise): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (g_conv_1x1_expand): Sequential(\n",
       "        (conv1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (ShuffleUnit_Stage4_3): ShuffleUnit(\n",
       "      (g_conv_1x1_compress): Sequential(\n",
       "        (conv1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (depthwise_conv3x3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "      (bn_after_depthwise): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (g_conv_1x1_expand): Sequential(\n",
       "        (conv1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (batch_norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2300, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = ShuffleNet()\n",
    "network.load_state_dict(torch.load('./models_good_0.pth'))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "network.to(device)\n",
    "network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_img_list  = []\n",
    "for root, dictionaries, filenames in os.walk('test_classification/medium/'):\n",
    "    for filename in filenames:\n",
    "        img_path = os.path.join(root, filename)\n",
    "        test_img_list.append(img_path)\n",
    "        \n",
    "test_img_list.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4600\n"
     ]
    }
   ],
   "source": [
    "print(len(test_img_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class testDataset(Dataset):\n",
    "    def __init__(self, file_list):\n",
    "        self.file_list = file_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.file_list[index])\n",
    "        img = torchvision.transforms.ToTensor()(img)\n",
    "        \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testset = testDataset(test_img_list)\n",
    "test_dataLoader = DataLoader(testset, batch_size=1, shuffle=False, num_workers=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4600\n"
     ]
    }
   ],
   "source": [
    "print(len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_embedding_resnet = []\n",
    "for batch_num, (feats) in enumerate(test_dataLoader):\n",
    "    feats = feats.to(device)\n",
    "    output = network(feats)\n",
    "    embedding = np.copy(output.cpu().detach().numpy())\n",
    "    test_embedding_resnet.append(embedding)\n",
    "    \n",
    "    del feats  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4600, 512)\n"
     ]
    }
   ],
   "source": [
    "test_embedding_save = np.zeros((1,512))\n",
    "for i in range(len(test_embedding_resnet)):\n",
    "    test_embedding_save = np.concatenate((test_embedding_save,test_embedding_resnet[i]),axis = 0)\n",
    "    \n",
    "test_embedding_save = test_embedding_save[1:,:]\n",
    "print(test_embedding_save.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4600\n"
     ]
    }
   ],
   "source": [
    "print(len(test_embedding_resnet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('shufflenet_test_embedding.npy',test_embedding_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
