{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "'''\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "from torch.nn import init\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PDataset(Dataset):\n",
    "    def __init__(self, data_file_list, label_file_list):\n",
    "        self.data_file_list = data_file_list\n",
    "        self.label_file_list = label_file_list\n",
    "        \n",
    "        self.data1 = np.load(data_file_list[0])\n",
    "        self.data2 = np.load(data_file_list[1])\n",
    "        self.data3 = np.load(data_file_list[2])\n",
    "        self.label = np.load(label_file_list[0])\n",
    "        \n",
    "        #self.label2 = np.load(label_file_list[1])\n",
    "        \n",
    "        #self.label3 = np.load(label_file_list[2])\n",
    "        \n",
    "        #self.n_class = len(np.unique(self.label))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data1.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        stack1 = np.hstack((self.data1[index], self.data2[index]))\n",
    "        embedding = np.hstack((stack1, self.data3[index]))\n",
    "        embedding = torch.from_numpy(embedding)\n",
    "        \n",
    "        label = self.label[index]\n",
    "        #label = torch.Tensor(label)\n",
    "        \n",
    "        return embedding, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset = PDataset([\"shufflenet_embedding.npy\", \"mobilenet_embedding.npy\", \"resnet_embedding.npy\"]\n",
    "                     ,[\"shufflenet_embedding_label.npy\",\"mobilenet_embedding_label.npy\",\"resnet_embedding_label.npy\"])\n",
    "train_dataloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=3,drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Classification Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Simple(nn.Module):\n",
    "    def __init__(self, n_in = 512*3, n_out = 2300):\n",
    "        super(Simple, self).__init__()\n",
    "        #self.conv1 = nn.Conv1d(in_channels=1, out_channels=256, kernel_size=8, stride=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(n_in, n_in*2, bias = True)\n",
    "        #self.fc1 = nn.Linear(256, n_in, bias = True)\n",
    "        self.dropout1 = nn.Dropout(p=0.1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(n_in*2)\n",
    "        #self.bn1 = nn.BatchNorm1d(n_in)\n",
    "        \n",
    "        self.fc2 = nn.Linear(n_in*2, n_in*4, bias = True)\n",
    "        #self.fc2 = nn.Linear(n_in, n_out, bias = True)\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm1d(n_in*4)\n",
    "\n",
    "        self.fc0 = nn.Linear(n_in*4, n_in*4, bias = True)\n",
    "        self.relu0 = nn.ReLU()\n",
    "        self.bn0 = nn.BatchNorm1d(n_in*4)\n",
    "        \n",
    "        self.fc3 = nn.Linear(n_in*4, n_out, bias = True)\n",
    "        #self.logprob = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        #x = x.unsqueeze(1)\n",
    "        #x = self.conv1(x)  #(N, C, K)\n",
    "        #x = x.mean([2])\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        x = self.fc0(x)\n",
    "        x = self.relu0(x)\n",
    "        x = self.bn0(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        #x = self.logprob(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple(\n",
      "  (fc1): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (relu1): ReLU()\n",
      "  (bn1): BatchNorm1d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=6144, bias=True)\n",
      "  (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  (relu2): ReLU()\n",
      "  (bn2): BatchNorm1d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc0): Linear(in_features=6144, out_features=6144, bias=True)\n",
      "  (relu0): ReLU()\n",
      "  (bn0): BatchNorm1d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=6144, out_features=2300, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "network = Simple()\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr = 5e-3)\n",
    "#optimizer = torch.optim.Adam(network.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is 0th iteration\n",
      "Epoch: 1\tBatch: 1000\tAvg-Loss: 5.7580\n",
      "Epoch: 1\tBatch: 2000\tAvg-Loss: 3.4206\n",
      "Epoch: 1\tBatch: 3000\tAvg-Loss: 2.5051\n",
      "Epoch: 1\tBatch: 4000\tAvg-Loss: 1.9984\n",
      "Epoch: 1\tBatch: 5000\tAvg-Loss: 1.6915\n",
      "Epoch: 1\tBatch: 6000\tAvg-Loss: 1.4922\n",
      "0.5564261683334266\n",
      "Epoch: 2\tBatch: 1000\tAvg-Loss: 1.2084\n",
      "Epoch: 2\tBatch: 2000\tAvg-Loss: 1.1260\n",
      "Epoch: 2\tBatch: 3000\tAvg-Loss: 1.0560\n",
      "Epoch: 2\tBatch: 4000\tAvg-Loss: 1.0030\n",
      "Epoch: 2\tBatch: 5000\tAvg-Loss: 0.9545\n",
      "Epoch: 2\tBatch: 6000\tAvg-Loss: 0.9085\n",
      "0.7993161865052046\n",
      "Epoch: 3\tBatch: 1000\tAvg-Loss: 0.7844\n",
      "Epoch: 3\tBatch: 2000\tAvg-Loss: 0.7670\n",
      "Epoch: 3\tBatch: 3000\tAvg-Loss: 0.7568\n",
      "Epoch: 3\tBatch: 4000\tAvg-Loss: 0.7358\n",
      "Epoch: 3\tBatch: 5000\tAvg-Loss: 0.7146\n",
      "Epoch: 3\tBatch: 6000\tAvg-Loss: 0.6996\n",
      "0.8506228760061983\n",
      "Epoch: 4\tBatch: 1000\tAvg-Loss: 0.6112\n",
      "Epoch: 4\tBatch: 2000\tAvg-Loss: 0.6050\n",
      "Epoch: 4\tBatch: 3000\tAvg-Loss: 0.5969\n",
      "Epoch: 4\tBatch: 4000\tAvg-Loss: 0.5898\n",
      "Epoch: 4\tBatch: 5000\tAvg-Loss: 0.5870\n",
      "Epoch: 4\tBatch: 6000\tAvg-Loss: 0.5772\n",
      "0.8791272681273825\n",
      "Epoch: 5\tBatch: 1000\tAvg-Loss: 0.5040\n",
      "Epoch: 5\tBatch: 2000\tAvg-Loss: 0.4978\n",
      "Epoch: 5\tBatch: 3000\tAvg-Loss: 0.4968\n",
      "Epoch: 5\tBatch: 4000\tAvg-Loss: 0.4984\n",
      "Epoch: 5\tBatch: 5000\tAvg-Loss: 0.4984\n",
      "Epoch: 5\tBatch: 6000\tAvg-Loss: 0.4941\n",
      "0.8990420286223749\n",
      "Epoch: 6\tBatch: 1000\tAvg-Loss: 0.4251\n",
      "Epoch: 6\tBatch: 2000\tAvg-Loss: 0.4270\n",
      "Epoch: 6\tBatch: 3000\tAvg-Loss: 0.4242\n",
      "Epoch: 6\tBatch: 4000\tAvg-Loss: 0.4281\n",
      "Epoch: 6\tBatch: 5000\tAvg-Loss: 0.4262\n",
      "Epoch: 6\tBatch: 6000\tAvg-Loss: 0.4242\n",
      "0.9146242674730038\n",
      "Epoch: 7\tBatch: 1000\tAvg-Loss: 0.3653\n",
      "Epoch: 7\tBatch: 2000\tAvg-Loss: 0.3659\n",
      "Epoch: 7\tBatch: 3000\tAvg-Loss: 0.3689\n",
      "Epoch: 7\tBatch: 4000\tAvg-Loss: 0.3701\n",
      "Epoch: 7\tBatch: 5000\tAvg-Loss: 0.3710\n",
      "Epoch: 7\tBatch: 6000\tAvg-Loss: 0.3678\n",
      "0.9269504253461031\n",
      "Epoch: 8\tBatch: 1000\tAvg-Loss: 0.3139\n",
      "Epoch: 8\tBatch: 2000\tAvg-Loss: 0.3200\n",
      "Epoch: 8\tBatch: 3000\tAvg-Loss: 0.3199\n",
      "Epoch: 8\tBatch: 4000\tAvg-Loss: 0.3217\n",
      "Epoch: 8\tBatch: 5000\tAvg-Loss: 0.3207\n",
      "Epoch: 8\tBatch: 6000\tAvg-Loss: 0.3255\n",
      "0.9382646073606647\n",
      "Epoch: 9\tBatch: 1000\tAvg-Loss: 0.2782\n",
      "Epoch: 9\tBatch: 2000\tAvg-Loss: 0.2789\n",
      "Epoch: 9\tBatch: 3000\tAvg-Loss: 0.2767\n",
      "Epoch: 9\tBatch: 4000\tAvg-Loss: 0.2834\n",
      "Epoch: 9\tBatch: 5000\tAvg-Loss: 0.2851\n",
      "Epoch: 9\tBatch: 6000\tAvg-Loss: 0.2845\n",
      "0.9472239993967067\n",
      "Epoch: 10\tBatch: 1000\tAvg-Loss: 0.2360\n",
      "Epoch: 10\tBatch: 2000\tAvg-Loss: 0.2431\n",
      "Epoch: 10\tBatch: 3000\tAvg-Loss: 0.2458\n",
      "Epoch: 10\tBatch: 4000\tAvg-Loss: 0.2478\n",
      "Epoch: 10\tBatch: 5000\tAvg-Loss: 0.2533\n",
      "Epoch: 10\tBatch: 6000\tAvg-Loss: 0.2528\n",
      "0.9553623773648245\n",
      "Epoch: 11\tBatch: 1000\tAvg-Loss: 0.2091\n",
      "Epoch: 11\tBatch: 2000\tAvg-Loss: 0.2145\n",
      "Epoch: 11\tBatch: 3000\tAvg-Loss: 0.2146\n",
      "Epoch: 11\tBatch: 4000\tAvg-Loss: 0.2189\n",
      "Epoch: 11\tBatch: 5000\tAvg-Loss: 0.2222\n",
      "Epoch: 11\tBatch: 6000\tAvg-Loss: 0.2228\n",
      "0.9626335212137872\n",
      "Epoch: 12\tBatch: 1000\tAvg-Loss: 0.1871\n",
      "Epoch: 12\tBatch: 2000\tAvg-Loss: 0.1886\n",
      "Epoch: 12\tBatch: 3000\tAvg-Loss: 0.1884\n",
      "Epoch: 12\tBatch: 4000\tAvg-Loss: 0.1921\n",
      "Epoch: 12\tBatch: 5000\tAvg-Loss: 0.1964\n",
      "Epoch: 12\tBatch: 6000\tAvg-Loss: 0.1972\n",
      "0.9686774010708457\n",
      "Epoch: 13\tBatch: 1000\tAvg-Loss: 0.1640\n",
      "Epoch: 13\tBatch: 2000\tAvg-Loss: 0.1672\n",
      "Epoch: 13\tBatch: 3000\tAvg-Loss: 0.1673\n",
      "Epoch: 13\tBatch: 4000\tAvg-Loss: 0.1706\n",
      "Epoch: 13\tBatch: 5000\tAvg-Loss: 0.1728\n",
      "Epoch: 13\tBatch: 6000\tAvg-Loss: 0.1747\n",
      "0.9736375423582443\n",
      "Epoch: 14\tBatch: 1000\tAvg-Loss: 0.1431\n",
      "Epoch: 14\tBatch: 2000\tAvg-Loss: 0.1467\n",
      "Epoch: 14\tBatch: 3000\tAvg-Loss: 0.1500\n",
      "Epoch: 14\tBatch: 4000\tAvg-Loss: 0.1514\n",
      "Epoch: 14\tBatch: 5000\tAvg-Loss: 0.1503\n",
      "Epoch: 14\tBatch: 6000\tAvg-Loss: 0.1522\n",
      "0.9784310481977829\n",
      "Epoch: 15\tBatch: 1000\tAvg-Loss: 0.1274\n",
      "Epoch: 15\tBatch: 2000\tAvg-Loss: 0.1286\n",
      "Epoch: 15\tBatch: 3000\tAvg-Loss: 0.1327\n",
      "Epoch: 15\tBatch: 4000\tAvg-Loss: 0.1325\n",
      "Epoch: 15\tBatch: 5000\tAvg-Loss: 0.1358\n",
      "Epoch: 15\tBatch: 6000\tAvg-Loss: 0.1363\n",
      "0.9823475893810648\n",
      "Epoch: 16\tBatch: 1000\tAvg-Loss: 0.1125\n",
      "Epoch: 16\tBatch: 2000\tAvg-Loss: 0.1141\n",
      "Epoch: 16\tBatch: 3000\tAvg-Loss: 0.1174\n",
      "Epoch: 16\tBatch: 4000\tAvg-Loss: 0.1198\n",
      "Epoch: 16\tBatch: 5000\tAvg-Loss: 0.1196\n",
      "Epoch: 16\tBatch: 6000\tAvg-Loss: 0.1214\n",
      "0.9854686591563138\n",
      "Epoch: 17\tBatch: 1000\tAvg-Loss: 0.0994\n",
      "Epoch: 17\tBatch: 2000\tAvg-Loss: 0.1022\n",
      "Epoch: 17\tBatch: 3000\tAvg-Loss: 0.1032\n",
      "Epoch: 17\tBatch: 4000\tAvg-Loss: 0.1064\n",
      "Epoch: 17\tBatch: 5000\tAvg-Loss: 0.1073\n",
      "Epoch: 17\tBatch: 6000\tAvg-Loss: 0.1081\n",
      "0.9882759191100451\n",
      "Epoch: 18\tBatch: 1000\tAvg-Loss: 0.0919\n",
      "Epoch: 18\tBatch: 2000\tAvg-Loss: 0.0922\n",
      "Epoch: 18\tBatch: 3000\tAvg-Loss: 0.0928\n",
      "Epoch: 18\tBatch: 4000\tAvg-Loss: 0.0947\n",
      "Epoch: 18\tBatch: 5000\tAvg-Loss: 0.0962\n",
      "Epoch: 18\tBatch: 6000\tAvg-Loss: 0.0965\n",
      "0.9902950055585693\n",
      "Epoch: 19\tBatch: 1000\tAvg-Loss: 0.0813\n",
      "Epoch: 19\tBatch: 2000\tAvg-Loss: 0.0832\n",
      "Epoch: 19\tBatch: 3000\tAvg-Loss: 0.0832\n",
      "Epoch: 19\tBatch: 4000\tAvg-Loss: 0.0846\n",
      "Epoch: 19\tBatch: 5000\tAvg-Loss: 0.0860\n",
      "Epoch: 19\tBatch: 6000\tAvg-Loss: 0.0871\n",
      "0.9922386803445583\n",
      "Epoch: 20\tBatch: 1000\tAvg-Loss: 0.0725\n",
      "Epoch: 20\tBatch: 2000\tAvg-Loss: 0.0741\n",
      "Epoch: 20\tBatch: 3000\tAvg-Loss: 0.0764\n",
      "Epoch: 20\tBatch: 4000\tAvg-Loss: 0.0768\n",
      "Epoch: 20\tBatch: 5000\tAvg-Loss: 0.0782\n",
      "Epoch: 20\tBatch: 6000\tAvg-Loss: 0.0794\n",
      "0.9936812324698292\n",
      "this is 1th iteration\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-331a46bfba00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m999\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "network.to(device)\n",
    "network.train()\n",
    "for i in range(10):\n",
    "    print(\"this is \"+str(i)+\"th iteration\")\n",
    "    for epoch in range(20):\n",
    "        avg_loss = 0\n",
    "        accuracy = 0\n",
    "        total = 0\n",
    "        for batch_num, (data, label) in enumerate(train_dataloader):\n",
    "\n",
    "            #data = Variable(data.float(), requires_grad = False)\n",
    "            #label = Variable(label.long(), requires_grad = False)\n",
    "            #print(label.size())\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = network(data.float())\n",
    "\n",
    "            loss = criterion(pred, label.view(-1).long())\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "            if batch_num%1000 == 999:\n",
    "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/1000))\n",
    "                avg_loss = 0.0\n",
    "                \n",
    "            _, pred_labels = torch.max(F.softmax(pred, dim=1), 1)\n",
    "            pred_labels = pred_labels.view(-1)\n",
    "            \n",
    "            accuracy += torch.sum(torch.eq(pred_labels, label)).item()\n",
    "            total += len(label)\n",
    "            \n",
    "            del data\n",
    "            del label\n",
    "            del loss\n",
    "        print(accuracy / total)\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(network.state_dict(),'embedding_MLP4.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Simple(\n",
       "  (fc1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (bn1): BatchNorm1d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=3072, out_features=6144, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (bn2): BatchNorm1d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc3): Linear(in_features=6144, out_features=2300, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.modules of Simple(\n",
      "  (fc1): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (bn1): BatchNorm1d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=6144, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (bn2): BatchNorm1d(6144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=6144, out_features=2300, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(network.modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network.load_state_dict(torch.load('embedding_MLP3.pth'))\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is 0th iteration\n",
      "Epoch: 1\tBatch: 1000\tAvg-Loss: 0.2351\n",
      "Epoch: 1\tBatch: 2000\tAvg-Loss: 0.2305\n",
      "Epoch: 1\tBatch: 3000\tAvg-Loss: 0.2288\n",
      "Epoch: 1\tBatch: 4000\tAvg-Loss: 0.2272\n",
      "Epoch: 1\tBatch: 5000\tAvg-Loss: 0.2264\n",
      "Epoch: 1\tBatch: 6000\tAvg-Loss: 0.2276\n",
      "0.9498208364856219\n",
      "Epoch: 2\tBatch: 1000\tAvg-Loss: 0.2156\n",
      "Epoch: 2\tBatch: 2000\tAvg-Loss: 0.2210\n",
      "Epoch: 2\tBatch: 3000\tAvg-Loss: 0.2214\n",
      "Epoch: 2\tBatch: 4000\tAvg-Loss: 0.2191\n",
      "Epoch: 2\tBatch: 5000\tAvg-Loss: 0.2213\n",
      "Epoch: 2\tBatch: 6000\tAvg-Loss: 0.2210\n",
      "0.952022370504796\n",
      "Epoch: 3\tBatch: 1000\tAvg-Loss: 0.2138\n",
      "Epoch: 3\tBatch: 2000\tAvg-Loss: 0.2106\n",
      "Epoch: 3\tBatch: 3000\tAvg-Loss: 0.2150\n",
      "Epoch: 3\tBatch: 4000\tAvg-Loss: 0.2156\n",
      "Epoch: 3\tBatch: 5000\tAvg-Loss: 0.2153\n",
      "Epoch: 3\tBatch: 6000\tAvg-Loss: 0.2133\n",
      "0.9537081860575999\n",
      "Epoch: 4\tBatch: 1000\tAvg-Loss: 0.2080\n",
      "Epoch: 4\tBatch: 2000\tAvg-Loss: 0.2136\n",
      "Epoch: 4\tBatch: 3000\tAvg-Loss: 0.2115\n",
      "Epoch: 4\tBatch: 4000\tAvg-Loss: 0.2080\n",
      "Epoch: 4\tBatch: 5000\tAvg-Loss: 0.2105\n",
      "Epoch: 4\tBatch: 6000\tAvg-Loss: 0.2120\n",
      "0.9544987921970823\n",
      "Epoch: 5\tBatch: 1000\tAvg-Loss: 0.2069\n",
      "Epoch: 5\tBatch: 2000\tAvg-Loss: 0.2084\n",
      "Epoch: 5\tBatch: 3000\tAvg-Loss: 0.2067\n",
      "Epoch: 5\tBatch: 4000\tAvg-Loss: 0.2090\n",
      "Epoch: 5\tBatch: 5000\tAvg-Loss: 0.2059\n",
      "Epoch: 5\tBatch: 6000\tAvg-Loss: 0.2082\n",
      "0.9554353563930845\n",
      "Epoch: 6\tBatch: 1000\tAvg-Loss: 0.2016\n",
      "Epoch: 6\tBatch: 2000\tAvg-Loss: 0.2059\n",
      "Epoch: 6\tBatch: 3000\tAvg-Loss: 0.2025\n",
      "Epoch: 6\tBatch: 4000\tAvg-Loss: 0.2050\n",
      "Epoch: 6\tBatch: 5000\tAvg-Loss: 0.2071\n",
      "Epoch: 6\tBatch: 6000\tAvg-Loss: 0.2059\n",
      "0.9562673173152475\n",
      "Epoch: 7\tBatch: 1000\tAvg-Loss: 0.2012\n",
      "Epoch: 7\tBatch: 2000\tAvg-Loss: 0.2012\n",
      "Epoch: 7\tBatch: 3000\tAvg-Loss: 0.2023\n",
      "Epoch: 7\tBatch: 4000\tAvg-Loss: 0.2001\n",
      "Epoch: 7\tBatch: 5000\tAvg-Loss: 0.2017\n",
      "Epoch: 7\tBatch: 6000\tAvg-Loss: 0.2045\n",
      "0.9567574931217265\n",
      "Epoch: 8\tBatch: 1000\tAvg-Loss: 0.1989\n",
      "Epoch: 8\tBatch: 2000\tAvg-Loss: 0.1993\n",
      "Epoch: 8\tBatch: 3000\tAvg-Loss: 0.1980\n",
      "Epoch: 8\tBatch: 4000\tAvg-Loss: 0.1979\n",
      "Epoch: 8\tBatch: 5000\tAvg-Loss: 0.2014\n",
      "Epoch: 8\tBatch: 6000\tAvg-Loss: 0.2018\n",
      "0.9574848507700504\n",
      "Epoch: 9\tBatch: 1000\tAvg-Loss: 0.1963\n",
      "Epoch: 9\tBatch: 2000\tAvg-Loss: 0.1949\n",
      "Epoch: 9\tBatch: 3000\tAvg-Loss: 0.1967\n",
      "Epoch: 9\tBatch: 4000\tAvg-Loss: 0.1981\n",
      "Epoch: 9\tBatch: 5000\tAvg-Loss: 0.2008\n",
      "Epoch: 9\tBatch: 6000\tAvg-Loss: 0.1970\n",
      "0.9581063888273973\n",
      "Epoch: 10\tBatch: 1000\tAvg-Loss: 0.1919\n",
      "Epoch: 10\tBatch: 2000\tAvg-Loss: 0.1957\n",
      "Epoch: 10\tBatch: 3000\tAvg-Loss: 0.1924\n",
      "Epoch: 10\tBatch: 4000\tAvg-Loss: 0.1973\n",
      "Epoch: 10\tBatch: 5000\tAvg-Loss: 0.1954\n",
      "Epoch: 10\tBatch: 6000\tAvg-Loss: 0.1985\n",
      "0.9584688513344215\n",
      "Epoch: 11\tBatch: 1000\tAvg-Loss: 0.1942\n",
      "Epoch: 11\tBatch: 2000\tAvg-Loss: 0.1957\n",
      "Epoch: 11\tBatch: 3000\tAvg-Loss: 0.1939\n",
      "Epoch: 11\tBatch: 4000\tAvg-Loss: 0.1927\n",
      "Epoch: 11\tBatch: 5000\tAvg-Loss: 0.1919\n",
      "Epoch: 11\tBatch: 6000\tAvg-Loss: 0.1942\n",
      "0.958910374455394\n",
      "Epoch: 12\tBatch: 1000\tAvg-Loss: 0.1880\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-397b8a7f073a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m999\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "network.to(device)\n",
    "network.train()\n",
    "for i in range(10):\n",
    "    print(\"this is \"+str(i)+\"th iteration\")\n",
    "    for epoch in range(20):\n",
    "        avg_loss = 0\n",
    "        accuracy = 0\n",
    "        total = 0\n",
    "        for batch_num, (data, label) in enumerate(train_dataloader):\n",
    "\n",
    "            #data = Variable(data.float(), requires_grad = False)\n",
    "            #label = Variable(label.long(), requires_grad = False)\n",
    "            #print(label.size())\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = network(data.float())\n",
    "\n",
    "            loss = criterion(pred, label.view(-1).long())\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "            if batch_num%1000 == 999:\n",
    "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/1000))\n",
    "                avg_loss = 0.0\n",
    "                \n",
    "            _, pred_labels = torch.max(F.softmax(pred, dim=1), 1)\n",
    "            pred_labels = pred_labels.view(-1)\n",
    "            \n",
    "            accuracy += torch.sum(torch.eq(pred_labels, label)).item()\n",
    "            total += len(label)\n",
    "            \n",
    "            del data\n",
    "            del label\n",
    "            del loss\n",
    "        print(accuracy / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
