{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Resnet for face recognition\n",
    "\n",
    "MLAI Project\n",
    "\n",
    "yidongh@andrew.cmu.edu\n",
    "\n",
    "2019-10-31\n",
    "'''\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the architecture\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation,\n",
    "                    groups=groups, bias = False, dilation=dilation)\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    __constants__ = ['downsample']\n",
    "    \n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                base_width=64, dilation=1, norm_layer = None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups!=1 or base_width!=64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "            \n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True) # Notice this operation. BatchNorm doesn't need its output to do back-prop. Bui conv2d needs.\n",
    "        self.conv2 = conv3x3(planes,planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "            \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    __constants__ = ['downsample']\n",
    "    \n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width/64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width,planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "            \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes = 2300, zero_init_residual = False,\n",
    "                groups = 1, width_per_group = 64, replace_stride_with_dilation=None,\n",
    "                norm_layer = None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "        \n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be none\"\n",
    "                            \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        \n",
    "        # This could be modified. If stride = 2, then 32->16. Now I change it from 2 to 1.\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=1, stride=1, padding=0,\n",
    "                              bias = False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        # Maxpool might be unnecessary\n",
    "        #self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=1, dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        \n",
    "        # Initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.b2.weight, 0)\n",
    "                    \n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride!=1 or self.inplanes != planes*block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "            conv1x1(self.inplanes,planes*block.expansion,stride),\n",
    "            norm_layer(planes*block.expansion))\n",
    "            \n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        \n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        # x = self.maxpool(x)\n",
    "        #print('1',x.shape)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        #print('2',x.shape)\n",
    "        x = self.layer2(x)\n",
    "        #print('3',x.shape)\n",
    "        x = self.layer3(x)\n",
    "        #print('4',x.shape)\n",
    "        x = self.layer4(x)\n",
    "        #print('5',x.shape)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        #print('6',x.shape)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    forward = _forward\n",
    "    \n",
    "def _resnet(arch, block, layers, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    return model\n",
    "\n",
    "def resnet18(**kwargs):\n",
    "    return _resnet('resnet18', BasicBlock, [2,2,2,2],**kwargs)\n",
    "\n",
    "def resnet34(**kwargs):\n",
    "    return _resnet('resnet34', BasicBlock, [3,4,6,3],**kwargs)\n",
    "\n",
    "def resnet38(**kwargs):\n",
    "    return _resnet('resnet38', Bottleneck, [3,3,3,3],**kwargs)\n",
    "\n",
    "def resnet50(**kwargs):\n",
    "    return _resnet('resnet50', Bottleneck, [3,4,6,3],**kwargs)\n",
    "\n",
    "def resnet101(**kwargs):\n",
    "    return _resnet('resnet101', Bottleneck, [3,4,23,3],**kwargs)\n",
    "\n",
    "def resnet152(**kwargs):\n",
    "    return _resnet('resnet152', Bottleneck, [3,8,36,3],**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, file_list, target_list):\n",
    "        self.file_list = file_list\n",
    "        self.target_list = target_list\n",
    "        self.n_class = len(list(set(target_list)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.file_list[index])\n",
    "        img = torchvision.transforms.ToTensor()(img)\n",
    "        label = self.target_list[index]\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_data(datadir):\n",
    "    img_list = []\n",
    "    ID_list = []\n",
    "    uniqueID_list = []\n",
    "    for root, directories, filenames in os.walk(datadir):\n",
    "        b = -1\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.jpg'):\n",
    "                if(filename[0]+filename[1]=='._'):\n",
    "                    filename = filename[2:]\n",
    "                filei = os.path.join(root, filename)\n",
    "                img_list.append(filei)\n",
    "                ID_list.append(root.split('/')[-1])\n",
    "                a = root.split('/')[-1]\n",
    "                if a!=b:\n",
    "                    uniqueID_list.append(a)\n",
    "                b=a\n",
    "                \n",
    "    class_n = len(uniqueID_list)\n",
    "    target_dict = dict(zip(uniqueID_list, range(class_n)))\n",
    "    reverse_dict = dict(zip(range(class_n),uniqueID_list))\n",
    "    label_list = [target_dict[ID_key] for ID_key in ID_list]\n",
    "    \n",
    "    print('{}\\t\\t{}\\n{}\\t\\t{}'.format('#Images', '#Labels', len(img_list), len(set(label_list))))\n",
    "    return img_list, label_list, class_n, reverse_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Images\t\t#Labels\n",
      "822154\t\t2300\n",
      "#Images\t\t#Labels\n",
      "4601\t\t2300\n"
     ]
    }
   ],
   "source": [
    "train_img_list, train_label_list, train_class_n, reverse_dict = parse_data('train_data/medium')\n",
    "val_img_list,val_label_list, val_class_n, val_dict = parse_data('validation_classification/medium')\n",
    "#test_img_list, test_label_list, test_class_n, _ = parse_data('test_c')\n",
    "\n",
    "trainset = ImageDataset(train_img_list, train_label_list)\n",
    "valset = ImageDataset(val_img_list, val_label_list)\n",
    "#testset = ImageDataset(test_img_list, test_label_list)\n",
    "#testset = torchvision.datasets.ImageFolder(root='test_c/test_classification/',transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "train_dataloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=3,drop_last=False)\n",
    "val_dataloader = DataLoader(valset, batch_size=128, shuffle=False, num_workers=3)\n",
    "#test_dataloader = DataLoader(testset[0], batch_size=1, shuffle=False,num_workers=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CenterLoss + CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CenterLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        feat_dim (int): feature dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, feat_dim, device = torch.device('cuda')):\n",
    "        super(CenterLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feat_dim = feat_dim\n",
    "        self.device = device\n",
    "        self.centers = nn.Parameter(torch.randn(self.num_classes,self.feat_dim).to(self.device))\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: feature matrix with shape (batch_size, feat_dim).\n",
    "            labels: ground truth labels with shape (batch_size).\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
    "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
    "        distmat.addmm_(1, -2, x, self.centers.t())\n",
    "        \n",
    "        classes = torch.arange(self.num_classes).long().to(self.device)\n",
    "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
    "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
    "\n",
    "        dist = []\n",
    "        for i in range(batch_size):\n",
    "            value = distmat[i][mask[i]]\n",
    "            value = value.clamp(min=1e-12, max=1e+12) # for numerical stability\n",
    "            dist.append(value)\n",
    "        dist = torch.cat(dist)\n",
    "        loss = dist.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, data_loader, test_loader, task = 'Classification'):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(numEpochs):\n",
    "        avg_loss = 0\n",
    "        for batch_num, (feats, labels) in enumerate(data_loader):\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer_label.zero_grad()\n",
    "            optimizer_closs.zero_grad()\n",
    "            \n",
    "            feature, outputs = model(feats)\n",
    "            \n",
    "            l_loss = criterion_label(outputs, lables.long())\n",
    "            c_loss = criterion_closs(feature, labels.long())\n",
    "            \n",
    "            loss = l_loss + closs_weight*c_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer_label.step()\n",
    "            \n",
    "            for param in criterion_closs.parameters():\n",
    "                param.grad.data *= (1. / closs_weight)\n",
    "            optimizer_closs.step()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "            if batch_num % 1000 == 999:\n",
    "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/1000))\n",
    "                avg_loss = 0.0    \n",
    "        \n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "            del loss\n",
    "        \n",
    "        if task == 'Classification':\n",
    "            val_loss, val_acc = test_classify(model, test_loader)\n",
    "            train_loss, train_acc = test_classify(model, data_loader)\n",
    "            print('Train Loss: {:.4f}\\tTrain Accuracy: {:.4f}\\tVal Loss: {:.4f}\\tVal Accuracy: {:.4f}'.\n",
    "                  format(train_loss, train_acc, val_loss, val_acc))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train1(model, data_loader, test_loader, task = 'Classification'):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(numEpochs):\n",
    "        avg_loss = 0\n",
    "        for batch_num, (feats, labels) in enumerate(data_loader):\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer_label.zero_grad()\n",
    "            \n",
    "            outputs = model(feats)\n",
    "            \n",
    "            l_loss = criterion_label(outputs, labels.long())\n",
    "            \n",
    "            loss = l_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer_label.step()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "            if batch_num % 1000 == 999:\n",
    "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/1000))\n",
    "                avg_loss = 0.0    \n",
    "        \n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "            del loss\n",
    "        \n",
    "        if task == 'Classification':\n",
    "            val_loss, val_acc = test_classify(model, test_loader)\n",
    "            train_loss, train_acc = test_classify(model, data_loader)\n",
    "            print('Train Loss: {:.4f}\\tTrain Accuracy: {:.4f}\\tVal Loss: {:.4f}\\tVal Accuracy: {:.4f}'.\n",
    "                  format(train_loss, train_acc, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_classify(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    accuracy = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_num, (feats, labels) in enumerate(test_loader):\n",
    "        feats, labels = feats.to(device), labels.to(device)\n",
    "        outputs = model(feats) ####\n",
    "        \n",
    "        _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        \n",
    "        l_loss = criterion_label(outputs, labels.long())\n",
    "        #c_loss = criterion_closs(feature, labels.long())\n",
    "        loss = l_loss #+ closs_weight * c_loss\n",
    "        #loss = criterion(outputs, labels.long())\n",
    "        \n",
    "        accuracy += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "        total += len(labels)\n",
    "        test_loss.extend([loss.item()]*feats.size()[0])\n",
    "        del feats\n",
    "        del labels\n",
    "\n",
    "    model.train()\n",
    "    return np.mean(test_loss), accuracy/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_test_image_list, train_test_label_list, n_test_class = parse_data('train_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numEpochs = 3\n",
    "\n",
    "learningRate = 1e-2\n",
    "weightDecay = 8e-5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=2300, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "network = resnet38()\n",
    "print(network)\n",
    "#closs_weight = 1\n",
    "lr_cent = 0.5\n",
    "#criterion_closs = CenterLoss(num_classes = 2300, feat_dim = 64, device = device)\n",
    "criterion_label = nn.CrossEntropyLoss()\n",
    "optimizer_label = optim.Adam(network.parameters())\n",
    "#optimizer_closs = torch.optim.SGD(criterion_closs.parameters(), lr=lr_cent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.modules of ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=2300, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(network.modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is 0th iteration\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 10.91 GiB total capacity; 6.34 GiB already allocated; 25.06 MiB free; 124.26 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-cc6bf3ac504f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"this is \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"th iteration\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m#torch.save(network.state_dict(), './model10_epoch'+str((i+1)*2)+'.pth')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-b0bbfa764733>\u001b[0m in \u001b[0;36mtrain1\u001b[0;34m(model, data_loader, test_loader, task)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 10.91 GiB total capacity; 6.34 GiB already allocated; 25.06 MiB free; 124.26 MiB cached)"
     ]
    }
   ],
   "source": [
    "network.to(device)\n",
    "network.train()\n",
    "for i in range(10):\n",
    "    print(\"this is \"+str(i)+\"th iteration\")\n",
    "    train1(network, train_dataloader, val_dataloader)\n",
    "    #torch.save(network.state_dict(), './model10_epoch'+str((i+1)*2)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(network.state_dict(),'Resnet38_iteration1_epoch2_dropout.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2300, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_img_list  = []\n",
    "for root, dictionaries, filenames in os.walk('test_classification/medium/'):\n",
    "    for filename in filenames:\n",
    "        img_path = os.path.join(root, filename)\n",
    "        test_img_list.append(img_path)\n",
    "        \n",
    "test_img_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_classification/medium/5000.jpg', 'test_classification/medium/5001.jpg', 'test_classification/medium/5002.jpg', 'test_classification/medium/5003.jpg', 'test_classification/medium/5004.jpg', 'test_classification/medium/5005.jpg', 'test_classification/medium/5006.jpg', 'test_classification/medium/5007.jpg', 'test_classification/medium/5008.jpg', 'test_classification/medium/5009.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(test_img_list[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class testDataset(Dataset):\n",
    "    def __init__(self, file_list):\n",
    "        self.file_list = file_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.file_list[index])\n",
    "        img = torchvision.transforms.ToTensor()(img)\n",
    "        \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testset = testDataset(test_img_list)\n",
    "test_dataLoader = DataLoader(testset, batch_size=1, shuffle=False, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "for batch_num, (feats) in enumerate(test_dataLoader):\n",
    "    feats = feats.to(device)\n",
    "    prediction = network(feats)\n",
    "    _, pred_labels = torch.max(F.softmax(prediction, dim=1), 1)\n",
    "    pred_labels = pred_labels.view(-1)\n",
    "    pred_list.append(pred_labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n"
     ]
    }
   ],
   "source": [
    "print(pred_list[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_list = np.array(pred_list)\n",
    "real_list = np.zeros_like(pred_list)\n",
    "for i in range(pred_list.shape[0]):\n",
    "    real_list[i] = reverse_dict[int(pred_list[i])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '857', 1: '2071', 2: '575', 3: '819', 4: '2254', 5: '629', 6: '2136', 7: '1001', 8: '2230', 9: '480', 10: '1084', 11: '1303', 12: '1588', 13: '342', 14: '1884', 15: '1882', 16: '1408', 17: '745', 18: '1157', 19: '464', 20: '669', 21: '1393', 22: '1749', 23: '1271', 24: '2161', 25: '1576', 26: '1283', 27: '1493', 28: '708', 29: '1189', 30: '1196', 31: '2234', 32: '1535', 33: '499', 34: '642', 35: '2214', 36: '865', 37: '1459', 38: '422', 39: '696', 40: '245', 41: '474', 42: '1251', 43: '1496', 44: '1295', 45: '74', 46: '578', 47: '1626', 48: '2133', 49: '98', 50: '1128', 51: '500', 52: '513', 53: '110', 54: '1429', 55: '2073', 56: '1959', 57: '25', 58: '963', 59: '1262', 60: '954', 61: '813', 62: '1212', 63: '1207', 64: '2209', 65: '1777', 66: '1979', 67: '1044', 68: '782', 69: '469', 70: '128', 71: '1668', 72: '320', 73: '1936', 74: '1642', 75: '1059', 76: '12', 77: '170', 78: '326', 79: '767', 80: '2227', 81: '2027', 82: '951', 83: '1123', 84: '1093', 85: '1422', 86: '2141', 87: '1832', 88: '1745', 89: '1723', 90: '1573', 91: '528', 92: '1441', 93: '1587', 94: '418', 95: '938', 96: '2159', 97: '1264', 98: '1509', 99: '1537', 100: '2108', 101: '2293', 102: '1510', 103: '1695', 104: '1302', 105: '1628', 106: '968', 107: '518', 108: '1218', 109: '1380', 110: '2083', 111: '1293', 112: '1933', 113: '1690', 114: '15', 115: '1272', 116: '2139', 117: '5', 118: '440', 119: '226', 120: '1751', 121: '1321', 122: '2001', 123: '1707', 124: '1116', 125: '144', 126: '1651', 127: '1156', 128: '1278', 129: '17', 130: '10', 131: '610', 132: '1938', 133: '398', 134: '207', 135: '1829', 136: '1013', 137: '1406', 138: '804', 139: '2217', 140: '184', 141: '2036', 142: '835', 143: '1740', 144: '599', 145: '557', 146: '1090', 147: '140', 148: '1577', 149: '2164', 150: '61', 151: '1741', 152: '1313', 153: '2019', 154: '925', 155: '361', 156: '520', 157: '471', 158: '1132', 159: '251', 160: '810', 161: '1931', 162: '191', 163: '1220', 164: '2218', 165: '1925', 166: '1319', 167: '585', 168: '2273', 169: '1483', 170: '803', 171: '1801', 172: '525', 173: '2078', 174: '1455', 175: '393', 176: '1627', 177: '1734', 178: '2144', 179: '1113', 180: '1291', 181: '337', 182: '2148', 183: '1062', 184: '922', 185: '328', 186: '1681', 187: '2238', 188: '1853', 189: '1223', 190: '2155', 191: '808', 192: '2138', 193: '1714', 194: '1923', 195: '1756', 196: '286', 197: '50', 198: '647', 199: '623', 200: '1451', 201: '932', 202: '256', 203: '1381', 204: '2266', 205: '2158', 206: '985', 207: '511', 208: '754', 209: '42', 210: '1266', 211: '2125', 212: '1237', 213: '1572', 214: '886', 215: '2051', 216: '1191', 217: '2156', 218: '57', 219: '2109', 220: '448', 221: '541', 222: '595', 223: '904', 224: '549', 225: '1115', 226: '426', 227: '428', 228: '1980', 229: '419', 230: '1164', 231: '1484', 232: '1437', 233: '1016', 234: '1499', 235: '880', 236: '692', 237: '151', 238: '1644', 239: '1928', 240: '797', 241: '1926', 242: '903', 243: '33', 244: '157', 245: '1528', 246: '645', 247: '379', 248: '1338', 249: '1860', 250: '864', 251: '29', 252: '99', 253: '1951', 254: '1058', 255: '441', 256: '77', 257: '1609', 258: '861', 259: '1981', 260: '1027', 261: '1407', 262: '2189', 263: '148', 264: '2216', 265: '133', 266: '929', 267: '2145', 268: '1055', 269: '706', 270: '2067', 271: '95', 272: '2191', 273: '741', 274: '1622', 275: '378', 276: '482', 277: '1328', 278: '38', 279: '1608', 280: '1306', 281: '1254', 282: '155', 283: '1769', 284: '1043', 285: '1680', 286: '227', 287: '1175', 288: '1575', 289: '1520', 290: '1607', 291: '1106', 292: '1186', 293: '509', 294: '858', 295: '1296', 296: '392', 297: '805', 298: '1256', 299: '467', 300: '586', 301: '2029', 302: '1845', 303: '2037', 304: '1621', 305: '118', 306: '1118', 307: '1983', 308: '2183', 309: '1762', 310: '1195', 311: '608', 312: '2258', 313: '2208', 314: '958', 315: '1848', 316: '2107', 317: '2170', 318: '1466', 319: '844', 320: '21', 321: '1227', 322: '690', 323: '2093', 324: '1010', 325: '550', 326: '408', 327: '1033', 328: '1755', 329: '34', 330: '1117', 331: '192', 332: '1222', 333: '2196', 334: '394', 335: '504', 336: '1511', 337: '1664', 338: '1754', 339: '16', 340: '546', 341: '327', 342: '851', 343: '1967', 344: '468', 345: '689', 346: '1569', 347: '609', 348: '1892', 349: '396', 350: '1270', 351: '2013', 352: '1436', 353: '1371', 354: '59', 355: '2111', 356: '306', 357: '2283', 358: '1217', 359: '1022', 360: '1356', 361: '1243', 362: '351', 363: '346', 364: '1878', 365: '1425', 366: '445', 367: '845', 368: '80', 369: '1653', 370: '834', 371: '193', 372: '2025', 373: '2122', 374: '661', 375: '2135', 376: '1505', 377: '573', 378: '1500', 379: '1806', 380: '108', 381: '1701', 382: '1691', 383: '1764', 384: '1095', 385: '1871', 386: '413', 387: '1252', 388: '1086', 389: '702', 390: '345', 391: '1996', 392: '129', 393: '1783', 394: '340', 395: '2121', 396: '1323', 397: '788', 398: '2012', 399: '1410', 400: '53', 401: '2297', 402: '2298', 403: '628', 404: '395', 405: '885', 406: '2035', 407: '901', 408: '717', 409: '1611', 410: '2081', 411: '2097', 412: '206', 413: '1236', 414: '1793', 415: '1034', 416: '1347', 417: '855', 418: '1478', 419: '45', 420: '1891', 421: '375', 422: '1593', 423: '54', 424: '389', 425: '2281', 426: '1674', 427: '1334', 428: '190', 429: '487', 430: '2096', 431: '2086', 432: '335', 433: '1167', 434: '1341', 435: '2199', 436: '23', 437: '621', 438: '676', 439: '571', 440: '576', 441: '2057', 442: '1404', 443: '24', 444: '848', 445: '296', 446: '1992', 447: '1184', 448: '1822', 449: '2147', 450: '139', 451: '1704', 452: '1985', 453: '494', 454: '1847', 455: '2220', 456: '946', 457: '1092', 458: '1208', 459: '236', 460: '554', 461: '581', 462: '316', 463: '2053', 464: '1024', 465: '950', 466: '1888', 467: '2224', 468: '1160', 469: '1562', 470: '94', 471: '582', 472: '726', 473: '2010', 474: '2023', 475: '2249', 476: '1965', 477: '894', 478: '1808', 479: '1742', 480: '1685', 481: '508', 482: '1258', 483: '796', 484: '1937', 485: '2024', 486: '432', 487: '2154', 488: '2276', 489: '1312', 490: '580', 491: '1533', 492: '1169', 493: '1026', 494: '1880', 495: '2223', 496: '1482', 497: '1008', 498: '1552', 499: '14', 500: '744', 501: '1308', 502: '119', 503: '822', 504: '1616', 505: '1571', 506: '1700', 507: '208', 508: '224', 509: '866', 510: '639', 511: '917', 512: '1750', 513: '1686', 514: '412', 515: '593', 516: '994', 517: '2269', 518: '943', 519: '555', 520: '1240', 521: '380', 522: '1210', 523: '2168', 524: '28', 525: '186', 526: '927', 527: '1553', 528: '2262', 529: '1909', 530: '1274', 531: '1041', 532: '2044', 533: '2061', 534: '458', 535: '1448', 536: '247', 537: '1669', 538: '318', 539: '1630', 540: '399', 541: '252', 542: '1591', 543: '1789', 544: '843', 545: '401', 546: '1322', 547: '195', 548: '1063', 549: '558', 550: '829', 551: '83', 552: '1861', 553: '30', 554: '1960', 555: '2062', 556: '37', 557: '1079', 558: '969', 559: '127', 560: '1177', 561: '2206', 562: '665', 563: '1507', 564: '1585', 565: '1814', 566: '1930', 567: '668', 568: '1929', 569: '780', 570: '347', 571: '194', 572: '1417', 573: '1011', 574: '1903', 575: '604', 576: '2185', 577: '1432', 578: '47', 579: '1514', 580: '1970', 581: '801', 582: '2288', 583: '1100', 584: '1895', 585: '944', 586: '2060', 587: '349', 588: '1914', 589: '948', 590: '517', 591: '493', 592: '538', 593: '641', 594: '1636', 595: '1383', 596: '1705', 597: '1105', 598: '84', 599: '1709', 600: '1469', 601: '153', 602: '1694', 603: '1187', 604: '2006', 605: '1904', 606: '1400', 607: '1037', 608: '567', 609: '2261', 610: '1162', 611: '90', 612: '105', 613: '176', 614: '643', 615: '2172', 616: '2285', 617: '2204', 618: '114', 619: '2076', 620: '1731', 621: '438', 622: '1151', 623: '1286', 624: '1527', 625: '2015', 626: '977', 627: '1828', 628: '1224', 629: '1110', 630: '510', 631: '1292', 632: '143', 633: '2245', 634: '1692', 635: '1311', 636: '817', 637: '1250', 638: '1342', 639: '648', 640: '1497', 641: '79', 642: '1566', 643: '1354', 644: '657', 645: '131', 646: '11', 647: '1398', 648: '223', 649: '1467', 650: '205', 651: '1416', 652: '2241', 653: '2002', 654: '1819', 655: '241', 656: '410', 657: '673', 658: '381', 659: '196', 660: '1662', 661: '403', 662: '1826', 663: '556', 664: '2187', 665: '1233', 666: '307', 667: '315', 668: '1485', 669: '1579', 670: '2257', 671: '368', 672: '1276', 673: '4', 674: '86', 675: '1544', 676: '120', 677: '1656', 678: '940', 679: '238', 680: '63', 681: '1753', 682: '1964', 683: '1486', 684: '2079', 685: '449', 686: '1077', 687: '766', 688: '2167', 689: '2212', 690: '991', 691: '1057', 692: '1736', 693: '2186', 694: '1330', 695: '1786', 696: '1176', 697: '274', 698: '1952', 699: '802', 700: '1506', 701: '1000', 702: '2290', 703: '2243', 704: '1202', 705: '784', 706: '666', 707: '103', 708: '161', 709: '1015', 710: '936', 711: '544', 712: '1529', 713: '2075', 714: '1883', 715: '248', 716: '1825', 717: '1953', 718: '344', 719: '358', 720: '145', 721: '1716', 722: '1589', 723: '1840', 724: '2263', 725: '1758', 726: '2119', 727: '769', 728: '1489', 729: '420', 730: '353', 731: '2219', 732: '125', 733: '1066', 734: '620', 735: '1324', 736: '2055', 737: '1006', 738: '756', 739: '667', 740: '2166', 741: '234', 742: '2228', 743: '1042', 744: '485', 745: '162', 746: '624', 747: '1317', 748: '32', 749: '1075', 750: '1098', 751: '1239', 752: '949', 753: '283', 754: '736', 755: '1650', 756: '1969', 757: '2072', 758: '863', 759: '1738', 760: '93', 761: '601', 762: '168', 763: '1973', 764: '1263', 765: '451', 766: '1532', 767: '2022', 768: '1214', 769: '2047', 770: '216', 771: '1792', 772: '2221', 773: '56', 774: '1', 775: '627', 776: '1872', 777: '1568', 778: '2260', 779: '1519', 780: '1657', 781: '818', 782: '979', 783: '1364', 784: '1539', 785: '1944', 786: '663', 787: '22', 788: '1905', 789: '790', 790: '1974', 791: '405', 792: '1759', 793: '383', 794: '1761', 795: '962', 796: '1242', 797: '300', 798: '812', 799: '2039', 800: '579', 801: '2003', 802: '2181', 803: '660', 804: '998', 805: '9', 806: '1775', 807: '1127', 808: '13', 809: '684', 810: '0', 811: '82', 812: '1802', 813: '1830', 814: '755', 815: '1844', 816: '1048', 817: '1358', 818: '214', 819: '658', 820: '1430', 821: '135', 822: '169', 823: '2059', 824: '200', 825: '719', 826: '816', 827: '2259', 828: '64', 829: '603', 830: '1284', 831: '1546', 832: '2202', 833: '213', 834: '1956', 835: '1205', 836: '1182', 837: '459', 838: '1025', 839: '343', 840: '699', 841: '1966', 842: '304', 843: '2043', 844: '1596', 845: '709', 846: '1725', 847: '1862', 848: '453', 849: '1713', 850: '293', 851: '446', 852: '1531', 853: '2239', 854: '199', 855: '1675', 856: '859', 857: '1053', 858: '1518', 859: '165', 860: '1768', 861: '1760', 862: '2248', 863: '1431', 864: '1581', 865: '1121', 866: '2058', 867: '1949', 868: '6', 869: '188', 870: '587', 871: '1728', 872: '2134', 873: '442', 874: '1666', 875: '1456', 876: '1816', 877: '992', 878: '2236', 879: '1522', 880: '912', 881: '686', 882: '388', 883: '1386', 884: '325', 885: '806', 886: '1927', 887: '2233', 888: '569', 889: '370', 890: '1807', 891: '1893', 892: '1473', 893: '1248', 894: '774', 895: '1219', 896: '1766', 897: '1171', 898: '1241', 899: '1646', 900: '1542', 901: '1080', 902: '1776', 903: '1104', 904: '19', 905: '532', 906: '1457', 907: '1472', 908: '122', 909: '1147', 910: '416', 911: '1314', 912: '1739', 913: '1534', 914: '72', 915: '760', 916: '1163', 917: '896', 918: '1697', 919: '1726', 920: '1161', 921: '2082', 922: '2087', 923: '1131', 924: '626', 925: '2069', 926: '1056', 927: '1174', 928: '543', 929: '563', 930: '231', 931: '20', 932: '1873', 933: '515', 934: '926', 935: '1865', 936: '1894', 937: '1135', 938: '1688', 939: '714', 940: '2016', 941: '255', 942: '619', 943: '1259', 944: '115', 945: '210', 946: '1508', 947: '1592', 948: '117', 949: '1730', 950: '2038', 951: '1947', 952: '884', 953: '1624', 954: '264', 955: '431', 956: '1551', 957: '262', 958: '2256', 959: '92', 960: '1377', 961: '2244', 962: '332', 963: '565', 964: '867', 965: '1246', 966: '682', 967: '841', 968: '1411', 969: '1696', 970: '244', 971: '997', 972: '1412', 973: '137', 974: '972', 975: '1727', 976: '2180', 977: '630', 978: '2050', 979: '1771', 980: '488', 981: '1019', 982: '303', 983: '100', 984: '1798', 985: '1654', 986: '1526', 987: '387', 988: '2092', 989: '1924', 990: '718', 991: '1516', 992: '664', 993: '1715', 994: '1310', 995: '584', 996: '460', 997: '259', 998: '288', 999: '1447', 1000: '536', 1001: '1563', 1002: '1712', 1003: '964', 1004: '722', 1005: '1387', 1006: '763', 1007: '505', 1008: '222', 1009: '330', 1010: '2021', 1011: '40', 1012: '1198', 1013: '2098', 1014: '2294', 1015: '1028', 1016: '881', 1017: '1570', 1018: '1637', 1019: '35', 1020: '1683', 1021: '1720', 1022: '743', 1023: '1849', 1024: '716', 1025: '2253', 1026: '2070', 1027: '1040', 1028: '2117', 1029: '651', 1030: '254', 1031: '2128', 1032: '36', 1033: '854', 1034: '697', 1035: '1435', 1036: '625', 1037: '809', 1038: '1617', 1039: '1503', 1040: '878', 1041: '1178', 1042: '430', 1043: '1629', 1044: '734', 1045: '367', 1046: '1245', 1047: '1867', 1048: '41', 1049: '2207', 1050: '535', 1051: '1565', 1052: '146', 1053: '1023', 1054: '655', 1055: '1958', 1056: '27', 1057: '175', 1058: '2041', 1059: '2068', 1060: '1449', 1061: '2106', 1062: '1721', 1063: '291', 1064: '302', 1065: '1074', 1066: '928', 1067: '1071', 1068: '2132', 1069: '1917', 1070: '1279', 1071: '1353', 1072: '792', 1073: '1192', 1074: '478', 1075: '971', 1076: '1373', 1077: '2005', 1078: '1068', 1079: '1702', 1080: '273', 1081: '1139', 1082: '1275', 1083: '978', 1084: '1350', 1085: '1875', 1086: '777', 1087: '69', 1088: '1554', 1089: '1541', 1090: '1450', 1091: '187', 1092: '1580', 1093: '414', 1094: '1602', 1095: '1471', 1096: '2124', 1097: '1986', 1098: '1247', 1099: '1101', 1100: '1401', 1101: '113', 1102: '906', 1103: '2255', 1104: '1540', 1105: '1988', 1106: '654', 1107: '2242', 1108: '2143', 1109: '377', 1110: '209', 1111: '246', 1112: '983', 1113: '443', 1114: '1805', 1115: '764', 1116: '1120', 1117: '2295', 1118: '2032', 1119: '545', 1120: '1372', 1121: '305', 1122: '2031', 1123: '646', 1124: '1273', 1125: '836', 1126: '1491', 1127: '1440', 1128: '1836', 1129: '568', 1130: '1234', 1131: '242', 1132: '956', 1133: '1216', 1134: '365', 1135: '331', 1136: '1797', 1137: '1737', 1138: '2160', 1139: '1203', 1140: '495', 1141: '299', 1142: '314', 1143: '1663', 1144: '134', 1145: '583', 1146: '339', 1147: '427', 1148: '883', 1149: '152', 1150: '1495', 1151: '382', 1152: '1094', 1153: '439', 1154: '984', 1155: '1476', 1156: '1158', 1157: '905', 1158: '1752', 1159: '1606', 1160: '2171', 1161: '1899', 1162: '674', 1163: '842', 1164: '1978', 1165: '1906', 1166: '908', 1167: '1379', 1168: '1443', 1169: '1678', 1170: '1999', 1171: '522', 1172: '1179', 1173: '1780', 1174: '78', 1175: '1584', 1176: '477', 1177: '2126', 1178: '1298', 1179: '891', 1180: '1460', 1181: '1399', 1182: '1948', 1183: '1831', 1184: '1064', 1185: '278', 1186: '1257', 1187: '1465', 1188: '1842', 1189: '2270', 1190: '1468', 1191: '2176', 1192: '2188', 1193: '1995', 1194: '783', 1195: '1779', 1196: '2048', 1197: '738', 1198: '211', 1199: '1046', 1200: '1355', 1201: '703', 1202: '1640', 1203: '1213', 1204: '1020', 1205: '44', 1206: '1545', 1207: '183', 1208: '832', 1209: '634', 1210: '1360', 1211: '888', 1212: '1827', 1213: '1126', 1214: '73', 1215: '1945', 1216: '923', 1217: '729', 1218: '635', 1219: '1433', 1220: '1318', 1221: '653', 1222: '204', 1223: '1357', 1224: '1785', 1225: '1458', 1226: '1901', 1227: '739', 1228: '1961', 1229: '261', 1230: '406', 1231: '506', 1232: '1474', 1233: '2137', 1234: '704', 1235: '1809', 1236: '700', 1237: '1082', 1238: '1987', 1239: '683', 1240: '2250', 1241: '1405', 1242: '678', 1243: '1244', 1244: '1396', 1245: '26', 1246: '691', 1247: '1003', 1248: '1290', 1249: '421', 1250: '887', 1251: '130', 1252: '952', 1253: '1288', 1254: '275', 1255: '402', 1256: '276', 1257: '846', 1258: '772', 1259: '1085', 1260: '2292', 1261: '1261', 1262: '996', 1263: '1824', 1264: '2004', 1265: '1502', 1266: '1561', 1267: '1054', 1268: '1366', 1269: '1599', 1270: '1260', 1271: '924', 1272: '688', 1273: '462', 1274: '7', 1275: '1420', 1276: '123', 1277: '452', 1278: '202', 1279: '662', 1280: '2074', 1281: '1359', 1282: '824', 1283: '1672', 1284: '2265', 1285: '1421', 1286: '2065', 1287: '363', 1288: '2026', 1289: '1140', 1290: '1030', 1291: '2011', 1292: '1625', 1293: '1811', 1294: '622', 1295: '1316', 1296: '1900', 1297: '1717', 1298: '1843', 1299: '1530', 1300: '43', 1301: '2286', 1302: '70', 1303: '111', 1304: '2102', 1305: '1812', 1306: '1940', 1307: '572', 1308: '1885', 1309: '369', 1310: '856', 1311: '1138', 1312: '872', 1313: '679', 1314: '1635', 1315: '939', 1316: '1910', 1317: '1835', 1318: '712', 1319: '1073', 1320: '2090', 1321: '2279', 1322: '2184', 1323: '65', 1324: '2063', 1325: '811', 1326: '612', 1327: '1453', 1328: '2174', 1329: '1345', 1330: '742', 1331: '1428', 1332: '2095', 1333: '257', 1334: '1305', 1335: '833', 1336: '463', 1337: '271', 1338: '1097', 1339: '1984', 1340: '1285', 1341: '109', 1342: '523', 1343: '1620', 1344: '1641', 1345: '1603', 1346: '730', 1347: '644', 1348: '87', 1349: '1852', 1350: '2291', 1351: '561', 1352: '789', 1353: '1582', 1354: '1070', 1355: '521', 1356: '1912', 1357: '334', 1358: '1676', 1359: '1556', 1360: '156', 1361: '409', 1362: '1087', 1363: '2114', 1364: '456', 1365: '1710', 1366: '1874', 1367: '747', 1368: '2101', 1369: '2077', 1370: '713', 1371: '724', 1372: '1765', 1373: '987', 1374: '166', 1375: '553', 1376: '548', 1377: '429', 1378: '1183', 1379: '1309', 1380: '1329', 1381: '1614', 1382: '915', 1383: '2169', 1384: '85', 1385: '2165', 1386: '873', 1387: '1299', 1388: '171', 1389: '2130', 1390: '1639', 1391: '681', 1392: '2211', 1393: '473', 1394: '2240', 1395: '362', 1396: '1464', 1397: '1415', 1398: '1119', 1399: '1332', 1400: '243', 1401: '577', 1402: '1221', 1403: '2040', 1404: '1194', 1405: '1125', 1406: '918', 1407: '1559', 1408: '1102', 1409: '1255', 1410: '596', 1411: '1320', 1412: '1304', 1413: '947', 1414: '2271', 1415: '1170', 1416: '989', 1417: '725', 1418: '1325', 1419: '1494', 1420: '158', 1421: '1633', 1422: '1067', 1423: '1277', 1424: '212', 1425: '31', 1426: '1794', 1427: '159', 1428: '975', 1429: '1238', 1430: '862', 1431: '229', 1432: '1267', 1433: '1863', 1434: '758', 1435: '1487', 1436: '2091', 1437: '588', 1438: '1481', 1439: '2194', 1440: '2085', 1441: '564', 1442: '1375', 1443: '1144', 1444: '1231', 1445: '1378', 1446: '974', 1447: '672', 1448: '1598', 1449: '1729', 1450: '1655', 1451: '514', 1452: '1631', 1453: '107', 1454: '1199', 1455: '1772', 1456: '534', 1457: '986', 1458: '589', 1459: '2237', 1460: '786', 1461: '2200', 1462: '870', 1463: '2192', 1464: '1152', 1465: '1133', 1466: '2123', 1467: '1837', 1468: '1538', 1469: '1879', 1470: '1181', 1471: '217', 1472: '853', 1473: '560', 1474: '1188', 1475: '308', 1476: '1911', 1477: '1881', 1478: '1232', 1479: '58', 1480: '173', 1481: '154', 1482: '710', 1483: '1352', 1484: '1864', 1485: '350', 1486: '935', 1487: '2151', 1488: '1943', 1489: '2042', 1490: '174', 1491: '285', 1492: '2267', 1493: '75', 1494: '2162', 1495: '750', 1496: '301', 1497: '179', 1498: '371', 1499: '631', 1500: '2150', 1501: '101', 1502: '2146', 1503: '2120', 1504: '529', 1505: '1963', 1506: '62', 1507: '552', 1508: '638', 1509: '324', 1510: '785', 1511: '967', 1512: '1122', 1513: '433', 1514: '400', 1515: '1950', 1516: '263', 1517: '687', 1518: '1770', 1519: '825', 1520: '1649', 1521: '260', 1522: '390', 1523: '740', 1524: '1942', 1525: '879', 1526: '876', 1527: '1395', 1528: '955', 1529: '89', 1530: '1823', 1531: '71', 1532: '1869', 1533: '850', 1534: '2030', 1535: '1249', 1536: '8', 1537: '424', 1538: '1148', 1539: '2296', 1540: '435', 1541: '67', 1542: '1896', 1543: '2275', 1544: '1962', 1545: '198', 1546: '840', 1547: '1012', 1548: '871', 1549: '1735', 1550: '1143', 1551: '2222', 1552: '1130', 1553: '988', 1554: '1767', 1555: '1413', 1556: '920', 1557: '497', 1558: '1014', 1559: '2052', 1560: '265', 1561: '1693', 1562: '815', 1563: '1136', 1564: '215', 1565: '2104', 1566: '611', 1567: '1512', 1568: '1268', 1569: '1032', 1570: '1154', 1571: '1112', 1572: '1297', 1573: '2113', 1574: '1107', 1575: '1294', 1576: '167', 1577: '1089', 1578: '1265', 1579: '542', 1580: '598', 1581: '1134', 1582: '1333', 1583: '1168', 1584: '1029', 1585: '731', 1586: '336', 1587: '981', 1588: '540', 1589: '1185', 1590: '2008', 1591: '727', 1592: '1854', 1593: '219', 1594: '1838', 1595: '150', 1596: '455', 1597: '751', 1598: '907', 1599: '1226', 1600: '1618', 1601: '1918', 1602: '898', 1603: '425', 1604: '2153', 1605: '2163', 1606: '910', 1607: '570', 1608: '503', 1609: '698', 1610: '1623', 1611: '1746', 1612: '934', 1613: '1099', 1614: '1515', 1615: '1475', 1616: '233', 1617: '902', 1618: '2232', 1619: '1523', 1620: '1782', 1621: '993', 1622: '753', 1623: '1368', 1624: '1142', 1625: '1230', 1626: '1897', 1627: '1687', 1628: '1820', 1629: '2110', 1630: '559', 1631: '597', 1632: '492', 1633: '1149', 1634: '185', 1635: '313', 1636: '1643', 1637: '268', 1638: '720', 1639: '1463', 1640: '916', 1641: '551', 1642: '1078', 1643: '232', 1644: '373', 1645: '141', 1646: '957', 1647: '1349', 1648: '877', 1649: '701', 1650: '765', 1651: '237', 1652: '1954', 1653: '1784', 1654: '852', 1655: '1504', 1656: '2080', 1657: '1047', 1658: '1997', 1659: '1870', 1660: '147', 1661: '2175', 1662: '2009', 1663: '1438', 1664: '1706', 1665: '1743', 1666: '1439', 1667: '220', 1668: '602', 1669: '2195', 1670: '1331', 1671: '1193', 1672: '292', 1673: '1418', 1674: '2149', 1675: '1594', 1676: '547', 1677: '1137', 1678: '537', 1679: '218', 1680: '794', 1681: '1855', 1682: '404', 1683: '1560', 1684: '132', 1685: '1661', 1686: '909', 1687: '1645', 1688: '1586', 1689: '519', 1690: '1490', 1691: '616', 1692: '1718', 1693: '1558', 1694: '1388', 1695: '1612', 1696: '160', 1697: '694', 1698: '1103', 1699: '1300', 1700: '55', 1701: '1513', 1702: '1327', 1703: '1173', 1704: '1201', 1705: '1699', 1706: '372', 1707: '384', 1708: '1788', 1709: '1427', 1710: '1083', 1711: '900', 1712: '1839', 1713: '1698', 1714: '1004', 1715: '1165', 1716: '1898', 1717: '1567', 1718: '1501', 1719: '124', 1720: '341', 1721: '1065', 1722: '2299', 1723: '2231', 1724: '1052', 1725: '882', 1726: '1517', 1727: '1708', 1728: '249', 1729: '1660', 1730: '1774', 1731: '590', 1732: '2205', 1733: '762', 1734: '1392', 1735: '1550', 1736: '333', 1737: '2105', 1738: '1670', 1739: '1919', 1740: '356', 1741: '1773', 1742: '1200', 1743: '671', 1744: '889', 1745: '1993', 1746: '770', 1747: '295', 1748: '1658', 1749: '1583', 1750: '1050', 1751: '847', 1752: '355', 1753: '530', 1754: '2215', 1755: '172', 1756: '512', 1757: '1915', 1758: '479', 1759: '2182', 1760: '860', 1761: '1972', 1762: '1732', 1763: '2064', 1764: '591', 1765: '1281', 1766: '1124', 1767: '321', 1768: '1803', 1769: '1061', 1770: '1197', 1771: '874', 1772: '68', 1773: '360', 1774: '680', 1775: '781', 1776: '937', 1777: '982', 1778: '1376', 1779: '566', 1780: '613', 1781: '1307', 1782: '2129', 1783: '39', 1784: '1382', 1785: '1778', 1786: '733', 1787: '911', 1788: '1348', 1789: '258', 1790: '2127', 1791: '1665', 1792: '1225', 1793: '838', 1794: '507', 1795: '228', 1796: '1810', 1797: '921', 1798: '417', 1799: '2226', 1800: '1445', 1801: '2173', 1802: '1444', 1803: '2014', 1804: '1800', 1805: '1346', 1806: '1351', 1807: '323', 1808: '606', 1809: '1337', 1810: '1932', 1811: '828', 1812: '685', 1813: '1521', 1814: '2282', 1815: '531', 1816: '930', 1817: '457', 1818: '759', 1819: '1921', 1820: '516', 1821: '2045', 1822: '748', 1823: '1804', 1824: '2088', 1825: '2252', 1826: '1536', 1827: '1719', 1828: '656', 1829: '142', 1830: '632', 1831: '1477', 1832: '496', 1833: '1548', 1834: '723', 1835: '1282', 1836: '1072', 1837: '2152', 1838: '1799', 1839: '2034', 1840: '1336', 1841: '277', 1842: '269', 1843: '1524', 1844: '2179', 1845: '2251', 1846: '164', 1847: '2056', 1848: '897', 1849: '1479', 1850: '1129', 1851: '1166', 1852: '776', 1853: '1858', 1854: '995', 1855: '798', 1856: '1682', 1857: '773', 1858: '444', 1859: '1039', 1860: '178', 1861: '2178', 1862: '721', 1863: '945', 1864: '605', 1865: '267', 1866: '240', 1867: '76', 1868: '1886', 1869: '1957', 1870: '715', 1871: '1850', 1872: '636', 1873: '491', 1874: '2054', 1875: '637', 1876: '607', 1877: '1876', 1878: '465', 1879: '1747', 1880: '112', 1881: '1365', 1882: '1920', 1883: '1941', 1884: '1370', 1885: '1724', 1886: '1009', 1887: '472', 1888: '1111', 1889: '1841', 1890: '893', 1891: '707', 1892: '270', 1893: '1060', 1894: '2225', 1895: '287', 1896: '1748', 1897: '359', 1898: '189', 1899: '2190', 1900: '695', 1901: '149', 1902: '814', 1903: '1722', 1904: '1868', 1905: '1851', 1906: '1402', 1907: '481', 1908: '1172', 1909: '385', 1910: '931', 1911: '1335', 1912: '1866', 1913: '1998', 1914: '1315', 1915: '1971', 1916: '1605', 1917: '1339', 1918: '1076', 1919: '2278', 1920: '941', 1921: '197', 1922: '1796', 1923: '297', 1924: '1939', 1925: '959', 1926: '1206', 1927: '1180', 1928: '18', 1929: '322', 1930: '181', 1931: '2046', 1932: '230', 1933: '364', 1934: '1525', 1935: '2049', 1936: '869', 1937: '1414', 1938: '1369', 1939: '2118', 1940: '1419', 1941: '1547', 1942: '1190', 1943: '279', 1944: '486', 1945: '1834', 1946: '875', 1947: '650', 1948: '2198', 1949: '933', 1950: '1301', 1951: '1442', 1952: '2201', 1953: '163', 1954: '2094', 1955: '1424', 1956: '1049', 1957: '999', 1958: '1673', 1959: '1791', 1960: '391', 1961: '1409', 1962: '942', 1963: '1344', 1964: '1145', 1965: '1634', 1966: '1975', 1967: '728', 1968: '282', 1969: '1361', 1970: '1384', 1971: '1287', 1972: '693', 1973: '913', 1974: '386', 1975: '1601', 1976: '705', 1977: '1574', 1978: '2116', 1979: '1326', 1980: '502', 1981: '1613', 1982: '1922', 1983: '600', 1984: '423', 1985: '2028', 1986: '746', 1987: '1391', 1988: '778', 1989: '1946', 1990: '823', 1991: '2272', 1992: '2289', 1993: '1498', 1994: '407', 1995: '761', 1996: '649', 1997: '659', 1998: '483', 1999: '1488', 2000: '97', 2001: '2284', 2002: '329', 2003: '294', 2004: '1887', 2005: '366', 2006: '1394', 2007: '966', 2008: '1857', 2009: '990', 2010: '1889', 2011: '1619', 2012: '800', 2013: '732', 2014: '640', 2015: '102', 2016: '675', 2017: '91', 2018: '919', 2019: '48', 2020: '1069', 2021: '1597', 2022: '775', 2023: '633', 2024: '1908', 2025: '1108', 2026: '1253', 2027: '1790', 2028: '2018', 2029: '116', 2030: '615', 2031: '225', 2032: '1610', 2033: '1877', 2034: '1081', 2035: '1543', 2036: '280', 2037: '2274', 2038: '793', 2039: '2017', 2040: '1815', 2041: '961', 2042: '1045', 2043: '914', 2044: '1480', 2045: '899', 2046: '1035', 2047: '1982', 2048: '1462', 2049: '1031', 2050: '1470', 2051: '2264', 2052: '81', 2053: '1703', 2054: '46', 2055: '126', 2056: '1659', 2057: '1916', 2058: '892', 2059: '1096', 2060: '310', 2061: '1787', 2062: '1343', 2063: '711', 2064: '1757', 2065: '1159', 2066: '1211', 2067: '1452', 2068: '239', 2069: '787', 2070: '1340', 2071: '1280', 2072: '735', 2073: '357', 2074: '1434', 2075: '1677', 2076: '1017', 2077: '1818', 2078: '1269', 2079: '352', 2080: '2112', 2081: '250', 2082: '2247', 2083: '66', 2084: '1005', 2085: '2213', 2086: '1446', 2087: '338', 2088: '890', 2089: '2277', 2090: '88', 2091: '965', 2092: '2193', 2093: '1652', 2094: '779', 2095: '2066', 2096: '253', 2097: '1632', 2098: '617', 2099: '1492', 2100: '2103', 2101: '1088', 2102: '2115', 2103: '2235', 2104: '791', 2105: '1109', 2106: '180', 2107: '1968', 2108: '973', 2109: '1744', 2110: '489', 2111: '1763', 2112: '2000', 2113: '1781', 2114: '1423', 2115: '1549', 2116: '1018', 2117: '1389', 2118: '450', 2119: '849', 2120: '447', 2121: '1733', 2122: '799', 2123: '1595', 2124: '771', 2125: '2084', 2126: '272', 2127: '1150', 2128: '1934', 2129: '1994', 2130: '1604', 2131: '737', 2132: '960', 2133: '289', 2134: '1813', 2135: '677', 2136: '177', 2137: '96', 2138: '311', 2139: '895', 2140: '1821', 2141: '136', 2142: '826', 2143: '1902', 2144: '807', 2145: '1991', 2146: '524', 2147: '527', 2148: '1021', 2149: '354', 2150: '757', 2151: '1209', 2152: '868', 2153: '820', 2154: '2089', 2155: '1600', 2156: '574', 2157: '1684', 2158: '490', 2159: '2033', 2160: '1403', 2161: '51', 2162: '1647', 2163: '1153', 2164: '2020', 2165: '2197', 2166: '1679', 2167: '1907', 2168: '1859', 2169: '60', 2170: '470', 2171: '284', 2172: '2140', 2173: '1711', 2174: '614', 2175: '203', 2176: '461', 2177: '539', 2178: '266', 2179: '454', 2180: '980', 2181: '1374', 2182: '1557', 2183: '1091', 2184: '290', 2185: '235', 2186: '466', 2187: '52', 2188: '1913', 2189: '1229', 2190: '437', 2191: '976', 2192: '1390', 2193: '201', 2194: '970', 2195: '281', 2196: '436', 2197: '411', 2198: '670', 2199: '1977', 2200: '298', 2201: '1817', 2202: '1795', 2203: '1667', 2204: '821', 2205: '1555', 2206: '1385', 2207: '2229', 2208: '319', 2209: '376', 2210: '501', 2211: '526', 2212: '312', 2213: '1590', 2214: '415', 2215: '1833', 2216: '221', 2217: '1363', 2218: '594', 2219: '1989', 2220: '2157', 2221: '618', 2222: '1146', 2223: '830', 2224: '1461', 2225: '768', 2226: '1671', 2227: '1038', 2228: '1228', 2229: '2280', 2230: '839', 2231: '953', 2232: '2203', 2233: '1114', 2234: '2142', 2235: '1935', 2236: '1362', 2237: '1155', 2238: '1689', 2239: '827', 2240: '2210', 2241: '749', 2242: '837', 2243: '348', 2244: '1990', 2245: '1397', 2246: '309', 2247: '795', 2248: '1007', 2249: '1367', 2250: '1036', 2251: '2287', 2252: '1051', 2253: '1002', 2254: '2177', 2255: '533', 2256: '1976', 2257: '3', 2258: '1454', 2259: '831', 2260: '1141', 2261: '434', 2262: '1955', 2263: '1615', 2264: '2100', 2265: '562', 2266: '2268', 2267: '476', 2268: '317', 2269: '1648', 2270: '752', 2271: '138', 2272: '49', 2273: '121', 2274: '1204', 2275: '1856', 2276: '1638', 2277: '397', 2278: '182', 2279: '592', 2280: '1215', 2281: '374', 2282: '1890', 2283: '104', 2284: '498', 2285: '1289', 2286: '2099', 2287: '2246', 2288: '1846', 2289: '1235', 2290: '1578', 2291: '2', 2292: '1426', 2293: '484', 2294: '475', 2295: '1564', 2296: '2007', 2297: '106', 2298: '652', 2299: '2131'}\n"
     ]
    }
   ],
   "source": [
    "print(reverse_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 947]\n",
      " [ 317]\n",
      " [1277]\n",
      " [2115]\n",
      " [ 863]\n",
      " [ 384]\n",
      " [1484]\n",
      " [ 172]\n",
      " [ 702]\n",
      " [ 245]]\n"
     ]
    }
   ],
   "source": [
    "print(real_list[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"resnet34_iteration1.csv\",real_list,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299])\n"
     ]
    }
   ],
   "source": [
    "print(reverse_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values(['357', '1961', '1943', '387', '1320', '183', '1530', '1862', '1207', '1820', '296', '1935', '1126', '994', '971', '1274', '1023', '3', '1924', '2038', '433', '692', '2086', '1346', '2043', '70', '2030', '420', '1596', '1053', '236', '1103', '323', '396', '518', '438', '329', '1567', '1624', '201', '766', '1946', '890', '1478', '1069', '1605', '2157', '970', '909', '801', '268', '1834', '1726', '870', '881', '39', '318', '1172', '515', '278', '2037', '921', '910', '1363', '350', '2283', '777', '1398', '832', '1562', '1224', '1557', '1680', '104', '1026', '294', '513', '2294', '2218', '102', '2203', '740', '1142', '1220', '209', '635', '526', '1837', '121', '1713', '432', '739', '764', '495', '1760', '861', '1302', '2050', '1221', '1213', '1440', '858', '763', '544', '312', '1148', '2065', '1013', '1183', '1452', '1008', '1981', '914', '1888', '113', '779', '1604', '2019', '1656', '1558', '2117', '1949', '860', '1780', '446', '637', '412', '347', '123', '556', '439', '1695', '234', '648', '322', '2237', '1227', '493', '415', '685', '1749', '403', '987', '398', '352', '351', '1563', '503', '227', '1939', '1682', '1140', '306', '1621', '752', '1136', '1043', '725', '1736', '2046', '1795', '1065', '1350', '1265', '995', '793', '1609', '272', '1356', '1335', '1120', '554', '1465', '576', '905', '1803', '1793', '1578', '2132', '723', '1097', '1650', '1660', '9', '1481', '1115', '1216', '702', '1890', '2075', '2097', '1258', '1930', '1472', '1249', '448', '1819', '1920', '610', '389', '689', '2076', '1662', '384', '1456', '37', '455', '540', '1378', '782', '2020', '1649', '2041', '1362', '65', '553', '2185', '1994', '98', '1864', '47', '2253', '894', '802', '431', '1589', '2128', '603', '154', '1181', '776', '662', '45', '105', '34', '1744', '675', '376', '980', '435', '933', '1839', '1531', '378', '232', '787', '748', '714', '1855', '499', '1112', '1049', '2002', '2058', '1955', '2004', '2120', '902', '1934', '663', '2296', '346', '978', '1322', '2155', '1705', '283', '257', '1470', '131', '386', '127', '1659', '1291', '1592', '1658', '1476', '462', '437', '1484', '862', '1211', '1812', '1912', '1355', '284', '349', '2119', '1101', '1086', '1751', '2083', '239', '1767', '2023', '1463', '2197', '1422', '363', '1464', '279', '27', '441', '569', '2225', '2025', '2229', '917', '225', '633', '1311', '381', '1321', '2055', '1556', '2069', '163', '883', '423', '160', '1832', '1199', '1579', '1247', '1133', '2091', '2102', '924', '1891', '555', '2100', '1666', '1698', '2212', '2219', '471', '984', '380', '789', '736', '523', '1418', '482', '677', '726', '270', '69', '632', '1377', '1138', '1969', '942', '1490', '4', '1017', '86', '1921', '734', '607', '604', '1353', '1054', '1874', '2165', '1753', '1208', '338', '737', '1807', '253', '629', '1980', '2278', '324', '2154', '965', '1246', '2189', '1152', '2263', '2121', '445', '62', '1863', '1915', '305', '1457', '82', '1288', '691', '1102', '328', '133', '277', '1667', '1116', '1124', '1309', '1551', '1814', '280', '2204', '778', '295', '974', '2042', '1948', '2051', '2009', '2156', '126', '1005', '162', '1296', '200', '2057', '1965', '1595', '1018', '1931', '57', '302', '795', '245', '680', '393', '359', '1232', '2284', '594', '1979', '1304', '1524', '938', '2173', '1527', '639', '1630', '289', '2235', '1132', '385', '1900', '1318', '100', '454', '304', '44', '831', '2136', '745', '871', '1000', '790', '504', '1590', '596', '2210', '1823', '158', '589', '1875', '873', '440', '746', '1651', '773', '10', '1586', '1597', '1866', '2008', '401', '297', '565', '266', '1818', '664', '330', '137', '356', '107', '1235', '1242', '1936', '1999', '1059', '181', '1857', '1850', '814', '1897', '74', '1052', '2202', '613', '535', '417', '2169', '1185', '1143', '2201', '1554', '2299', '874', '1158', '1975', '391', '1046', '1883', '1342', '421', '2090', '792', '29', '668', '221', '2152', '1134', '327', '497', '878', '217', '2139', '892', '1300', '545', '1743', '370', '1859', '2170', '619', '1538', '2010', '2040', '1906', '1575', '120', '1460', '1427', '699', '1217', '1741', '1536', '138', '170', '1226', '490', '334', '1739', '1914', '1670', '1714', '1830', '1871', '1409', '1750', '936', '1622', '402', '2113', '124', '216', '1497', '1574', '597', '940', '2239', '2074', '1011', '2258', '991', '125', '1917', '2246', '1325', '79', '926', '1602', '1685', '537', '374', '642', '1230', '2236', '1109', '1508', '708', '251', '1330', '929', '1947', '193', '1618', '8', '2027', '1786', '397', '682', '1234', '643', '1161', '1415', '713', '122', '1383', '590', '690', '255', '1645', '1385', '1323', '1063', '1405', '2190', '494', '1982', '345', '1122', '379', '7', '1697', '1916', '2233', '473', '399', '847', '1455', '1366', '426', '1087', '1432', '1098', '1203', '1899', '1636', '274', '176', '325', '2221', '30', '213', '1003', '1287', '1493', '536', '265', '2054', '1663', '1166', '1700', '2182', '865', '986', '681', '2161', '1031', '136', '1212', '152', '1548', '968', '15', '1854', '469', '428', '1329', '982', '2', '1354', '1797', '2280', '1895', '1973', '1929', '488', '1125', '1927', '1571', '88', '735', '1083', '1800', '496', '811', '2241', '103', '1153', '180', '1306', '169', '1754', '249', '1783', '1573', '410', '1139', '1689', '1623', '1073', '618', '1118', '767', '76', '2013', '1428', '704', '551', '899', '342', '1190', '1156', '946', '466', '311', '2177', '1492', '1521', '609', '73', '2163', '1370', '542', '1077', '1525', '56', '1438', '751', '1634', '1699', '315', '1785', '191', '237', '1135', '1442', '1254', '1343', '1413', '1893', '1146', '1130', '1439', '33', '139', '2127', '1745', '1990', '1675', '1734', '2223', '2029', '1853', '579', '1668', '1942', '1219', '750', '1941', '872', '634', '2007', '1483', '1821', '1361', '930', '1944', '1519', '1061', '967', '314', '368', '1064', '1684', '1333', '1099', '487', '743', '1446', '1646', '826', '988', '2209', '828', '591', '816', '1193', '1506', '2111', '563', '2172', '1215', '2104', '23', '343', '1789', '1923', '174', '99', '222', '806', '78', '1085', '1790', '1782', '2064', '1728', '1127', '275', '2176', '1601', '1665', '194', '531', '2078', '442', '335', '653', '2194', '83', '2261', '557', '1587', '1154', '1764', '80', '1374', '1998', '919', '1664', '449', '185', '2181', '1021', '267', '372', '1794', '1534', '2082', '90', '855', '1940', '605', '1657', '1033', '50', '1553', '2116', '1256', '319', '522', '1431', '2144', '1583', '599', '61', '896', '1238', '229', '1042', '1762', '1189', '2005', '621', '2228', '1552', '1006', '1010', '1694', '1549', '1259', '2192', '282', '290', '1348', '2252', '1771', '418', '254', '1475', '2124', '1262', '961', '273', '772', '533', '657', '2048', '479', '1477', '151', '1338', '1706', '1229', '1559', '1500', '747', '2098', '1887', '1028', '969', '1339', '93', '920', '744', '703', '63', '2031', '1205', '644', '655', '2079', '2145', '827', '1070', '1319', '1738', '705', '1727', '1683', '32', '310', '749', '2256', '1384', '1671', '1298', '1533', '1822', '935', '2062', '1447', '2281', '1037', '1192', '1962', '204', '1499', '1326', '1718', '164', '2129', '173', '1704', '937', '119', '2295', '1847', '233', '1285', '697', '1349', '1271', '1687', '1252', '1450', '344', '1870', '1332', '1453', '1194', '952', '998', '2191', '1817', '1811', '31', '517', '1167', '1652', '1040', '2214', '28', '1911', '641', '1635', '1165', '1654', '165', '667', '1614', '1775', '1067', '2024', '1843', '973', '1421', '2099', '1248', '568', '1449', '719', '762', '771', '1286', '299', '130', '796', '1831', '460', '884', '1779', '87', '108', '815', '1801', '208', '1826', '1244', '172', '676', '1395', '625', '2255', '1716', '1308', '309', '765', '962', '560', '2205', '916', '1746', '1983', '915', '1301', '2059', '365', '1806', '2167', '2044', '992', '235', '392', '1611', '2107', '258', '1347', '1451', '1976', '97', '2226', '694', '1149', '81', '1848', '1019', '983', '2180', '1802', '2222', '223', '939', '1787', '1564', '1905', '240', '1988', '1835', '1913', '1722', '1108', '394', '1260', '1989', '2085', '1631', '825', '701', '135', '595', '144', '546', '1057', '348', '1105', '1724', '951', '467', '425', '1482', '1171', '2287', '1676', '1035', '2036', '2187', '2245', '1310', '364', '416', '1747', '819', '463', '1522', '0', '1954', '1157', '2227', '794', '219', '1401', '243', '1174', '259', '502', '1327', '206', '1781', '2264', '1501', '2108', '36', '1708', '2196', '1555', '1641', '2126', '2061', '2006', '785', '852', '291', '669', '863', '1114', '1110', '948', '1898', '1840', '837', '1275', '1674', '1702', '1012', '1328', '16', '1937', '1488', '1881', '2071', '1267', '20', '506', '2195', '406', '464', '210', '1710', '956', '617', '1004', '1233', '371', '111', '1117', '1799', '1974', '1056', '2052', '944', '566', '1639', '52', '308', '580', '1836', '624', '620', '2014', '753', '1880', '414', '115', '717', '478', '134', '2240', '1080', '475', '400', '805', '456', '486', '276', '1225', '646', '75', '570', '1759', '1462', '202', '552', '244', '1411', '833', '2208', '718', '66', '889', '373', '1532', '1047', '1610', '891', '1294', '1390', '1591', '2242', '468', '800', '1196', '2273', '1619', '1858', '55', '729', '1882', '205', '2056', '608', '1111', '1707', '932', '1824', '1198', '1626', '1399', '1529', '1768', '1778', '2094', '1393', '106', '407', '623', '1945', '859', '727', '2060', '2081', '2247', '1261', '1243', '835', '1426', '1827', '1692', '1841', '1672', '774', '1036', '845', '1932', '1526', '199', '598', '1737', '1084', '1095', '844', '848', '1598', '2140', '1770', '575', '567', '2159', '196', '38', '584', '1131', '1776', '1678', '211', '157', '1627', '218', '1723', '142', '178', '836', '913', '1160', '51', '1585', '985', '1971', '880', '1072', '950', '242', '2153', '256', '1498', '1909', '1878', '1459', '760', '1581', '1281', '2289', '1661', '1951', '1106', '958', '46', '1184', '2073', '1473', '1569', '1633', '640', '1403', '168', '94', '652', '1829', '2039', '1119', '1833', '1640', '477', '775', '1163', '129', '2147', '2070', '1027', '666', '710', '1182', '167', '262', '1388', '1489', '1813', '908', '58', '13', '1978', '1958', '2133', '35', '1187', '501', '1761', '1681', '424', '1679', '1075', '92', '508', '2213', '238', '457', '1748', '1763', '293', '1877', '1396', '489', '955', '1273', '882', '2215', '1068', '1467', '1015', '1608', '820', '534', '1389', '500', '2290', '1599', '520', '1236', '1315', '529', '600', '1307', '885', '177', '6', '1144', '769', '1637', '972', '1992', '840', '1545', '818', '688', '638', '671', '1074', '175', '2291', '730', '1128', '461', '485', '2066', '85', '2171', '2123', '672', '803', '1987', '2288', '2265', '822', '2211', '678', '337', '1420', '1344', '2018', '179', '1686', '2243', '1885', '451', '2275', '1441', '947', '72', '480', '849', '459', '1255', '1412', '841', '2022', '1838', '2188', '190', '2109', '1200', '616', '706', '959', '476', '1755', '593', '866', '587', '64', '1020', '571', '1376', '1594', '1889', '687', '481', '1773', '1204', '636', '1245', '585', '1966', '228', '195', '2262', '2268', '1228', '1129', '2012', '1071', '447', '147', '96', '1096', '2103', '367', '474', '1701', '1507', '928', '846', '895', '514', '2028', '1725', '1058', '67', '2266', '2272', '140', '2282', '1241', '2216', '869', '606', '1093', '1856', '1798', '1733', '1742', '1612', '1565', '1632', '875', '626', '1331', '2001', '1815', '548', '2053', '521', '2130', '321', '141', '901', '1528', '1808', '1731', '532', '628', '355', '1188', '813', '808', '1406', '684', '1985', '263', '1351', '695', '2207', '198', '1312', '843', '1904', '110', '934', '1123', '2286', '696', '1170', '1179', '150', '1516', '1081', '26', '250', '1851', '1816', '2250', '1865', '303', '1251', '2138', '43', '212', '577', '1113', '1541', '1616', '1642', '1382', '1584', '622', '1371', '1202', '1568', '1469', '1861', '182', '60', '2080', '829', '1038', '1387', '341', '957', '543', '452', '1510', '903', '1155', '1504', '1693', '2183', '953', '89', '2142', '1603', '2248', '573', '559', '943', '2220', '631', '1218', '184', '976', '1886', '1145', '960', '830', '1938', '1872', '549', '1629', '1268', '527', '326', '1950', '996', '1417', '1638', '539', '1828', '879', '1270', '1264', '395', '1480', '116', '656', '2045', '84', '17', '2151', '693', '1045', '700', '148', '1846', '712', '109', '1606', '214', '226', '156', '77', '868', '2110', '382', '1316', '564', '2224', '59', '1186', '14', '612', '2277', '12', '665', '1206', '1486', '674', '989', '975', '812', '1025', '1511', '1960', '1740', '472', '307', '1076', '470', '2141', '450', '1425', '2271', '2000', '530', '2179', '510', '1419', '898', '252', '11', '2150', '143', '728', '2021', '261', '1963', '340', '660', '999', '260', '755', '1485', '1324', '1381', '1715', '155', '434', '1433', '366', '1991', '91', '1195', '1429', '2297', '1269', '188', '1544', '1894', '1151', '1952', '1240', '161', '1173', '1210', '649', '1314', '809', '904', '2230', '1201', '404', '197', '1468', '1279', '797', '1690', '207', '1496', '588', '1062', '1620', '906', '483', '405', '298', '1535', '581', '1523', '1756', '2149', '95', '1643', '780', '2274', '2092', '1002', '1461', '339', '945', '377', '907', '2137', '288', '1443', '189', '582', '1214', '1168', '670', '1379', '2244', '1041', '1984', '465', '353', '1290', '2114', '411', '409', '187', '300', '1295', '1902', '1231', '1769', '2217', '25', '2269', '388', '558', '1437', '422', '990', '224', '761', '316', '1986', '375', '68', '1550', '1400', '715', '1078', '857', '524', '2292', '1653', '248', '525', '1169', '1791', '320', '301', '171', '711', '2146', '132', '1394', '1263', '40', '2112', '1869', '1600', '1509', '1386', '427', '659', '1180', '1303', '651', '1237', '784', '1495', '1299', '897', '707', '1007', '864', '2033', '2175', '927', '647', '2160', '1397', '1434', '1289', '1503', '1048', '492', '886', '1572', '146', '1625', '1373', '2270', '2184', '1150', '768', '931', '1164', '1825', '498', '2101', '1730', '1364', '2174', '1100', '369', '128', '918', '186', '854', '1774', '1141', '1926', '2011', '1570', '1752', '1491', '876', '1414', '1907', '754', '1436', '2232', '390', '2095', '408', '912', '893', '1677', '358', '1852', '1928', '1933', '954', '963', '757', '491', '1957', '2125', '5', '2089', '788', '19', '1712', '1039', '1278', '781', '1788', '2026', '758', '241', '1849', '1582', '732', '683', '2158', '1375', '1014', '1615', '2047', '1177', '1655', '1903', '1546', '443', '2178', '1560', '1222', '18', '1845', '923', '1276', '1972', '1051', '1293', '2298', '979', '507', '1784', '1804', '1925', '2088', '1454', '48', '823', '2016', '724', '586', '1956', '269', '572', '1607', '900', '149', '2067', '453', '1445', '1792', '1765', '1711', '1410', '2164', '2206', '505', '1297', '1996', '1918', '1908', '360', '1121', '247', '1512', '602', '925', '817', '22', '997', '2131', '2093', '1416', '614', '1691', '1577', '1402', '547', '887', '1876', '1137', '2279', '791', '645', '1796', '1959', '1868', '436', '512', '1967', '1810', '2035', '1613', '118', '1593', '1088', '1867', '2143', '1066', '1466', '877', '1772', '246', '2077', '1176', '458', '1404', '738', '41', '1359', '698', '1842', '1380', '541', '673', '1758', '1360', '966', '601', '1505', '1282', '2199', '153', '1884', '1089', '516', '1993', '1391', '2034', '1424', '1964', '354', '1777', '2198', '1050', '1016', '783', '592', '686', '1995', '1284', '1001', '1719', '838', '159', '867', '192', '286', '2063', '2032', '2106', '1805', '842', '2249', '114', '2251', '561', '1561', '2231', '332', '804', '2087', '1175', '1352', '922', '1283', '679', '1266', '1407', '444', '2257', '2096', '1494', '1513', '1721', '2260', '1367', '1873', '721', '1673', '1191', '287', '1644', '1032', '2166', '2003', '215', '574', '1250', '336', '1879', '1471', '2276', '1709', '770', '583', '1253', '853', '1537', '511', '281', '941', '1539', '49', '1055', '949', '981', '1502', '1', '166', '1514', '2017', '1091', '2293', '807', '285', '42', '1090', '1479', '824', '2084', '733', '1092', '2148', '361', '1688', '71', '54', '1487', '1357', '1408', '2193', '1617', '1094', '1977', '716', '821', '1809', '627', '2115', '1317', '1365', '1997', '1474', '1272', '2254', '1703', '2238', '1369', '799', '2234', '786', '1458', '117', '1647', '1223', '2072', '271', '1717', '834', '1305', '977', '1444', '1280', '2135', '203', '756', '2186', '722', '313', '362', '1178', '1368', '1060', '383', '1540', '1082', '112', '220', '1844', '661', '798', '2015', '1735', '2267', '331', '1197', '720', '1104', '850', '231', '1029', '292', '1430', '1720', '1580', '709', '2285', '1757', '101', '851', '615', '1147', '810', '1313', '2134', '2068', '430', '1372', '484', '1953', '1358', '53', '1337', '611', '1892', '1517', '1922', '856', '2259', '1860', '993', '538', '1022', '1024', '230', '731', '2168', '1107', '21', '2049', '1345', '413', '1239', '1588', '888', '1732', '658', '528', '839', '2200', '1547', '630', '429', '1340', '1648', '145', '1277', '1009', '1628', '333', '1669', '509', '1257', '1896', '419', '1901', '1292', '1079', '1435', '2162', '1566', '1034', '1515', '1543', '1423', '1518', '1209', '964', '1448', '1919', '1044', '1968', '1970', '1696', '519', '2122', '1766', '911', '741', '1520', '562', '1576', '1910', '550', '1729', '759', '264', '1159', '1392', '1341', '1542', '650', '1162', '2118', '742', '578', '24', '1030', '1336', '654', '1334', '317', '2105'])\n"
     ]
    }
   ],
   "source": [
    "print(reverse_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6808130048"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
